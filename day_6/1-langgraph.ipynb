{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJXW_DgiSebM"
      },
      "source": [
        "# LangGraph and LangSmith - Agentic RAG Powered by LangChain\n",
        "\n",
        "In the following notebook we'll complete the following tasks:\n",
        "\n",
        "- ü§ù Breakout Room #1:\n",
        "  1. Install required libraries\n",
        "  2. Set Environment Variables\n",
        "  3. Creating our Tool Belt\n",
        "  4. Creating Our State\n",
        "  5. Creating and Compiling A Graph!\n",
        "\n",
        "- ü§ù Breakout Room #2:\n",
        "  1. Evaluating the LangGraph Application with LangSmith\n",
        "  2. Adding Helpfulness Check and \"Loop\" Limits\n",
        "  3. LangGraph for the \"Patterns\" of GenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djQ3nRAgoF67"
      },
      "source": [
        "# ü§ù Breakout Room #1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7pQDUhUnIo8"
      },
      "source": [
        "## Part 1: LangGraph - Building Cyclic Applications with LangChain\n",
        "\n",
        "LangGraph is a tool that leverages LangChain Expression Language to build coordinated multi-actor and stateful applications that includes cyclic behaviour.\n",
        "\n",
        "### Why Cycles?\n",
        "\n",
        "In essence, we can think of a cycle in our graph as a more robust and customizable loop. It allows us to keep our application agent-forward while still giving the powerful functionality of traditional loops.\n",
        "\n",
        "Due to the inclusion of cycles over loops, we can also compose rather complex flows through our graph in a much more readable and natural fashion. Effectively allowing us to recreate application flowcharts in code in an almost 1-to-1 fashion.\n",
        "\n",
        "### Why LangGraph?\n",
        "\n",
        "Beyond the agent-forward approach - we can easily compose and combine traditional \"DAG\" (directed acyclic graph) chains with powerful cyclic behaviour due to the tight integration with LCEL. This means it's a natural extension to LangChain's core offerings!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_fLDElOVoop"
      },
      "source": [
        "## Task 1:  Dependencies\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wujPjGJuoPwg"
      },
      "source": [
        "## Task 2: Environment Variables\n",
        "\n",
        "We'll want to set both our OpenAI API key and our LangSmith environment variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jdh8CoVWHRvs",
        "outputId": "3fa78560-393c-4ee5-b871-9886bf0d70f4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jkla2fpx28QK",
        "outputId": "52d7ad22-fcb1-4abe-853b-216c55a12650"
      },
      "outputs": [],
      "source": [
        "os.environ[\"TAVILY_API_KEY\"] = getpass.getpass(\"TAVILY_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nv0glIDyHmRt",
        "outputId": "b69df90a-b4e1-4ddb-9de0-882d98b68ab2"
      },
      "outputs": [],
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = f\"PSI AIE6 - LangGraph - {uuid4().hex[0:8]}\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"LangSmith API Key: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBRyQmEAVzua"
      },
      "source": [
        "## Task 3: Creating our Tool Belt\n",
        "\n",
        "As is usually the case, we'll want to equip our agent with a toolbelt to help answer questions and add external knowledge.\n",
        "\n",
        "There's a tonne of tools in the [LangChain Community Repo](https://github.com/langchain-ai/langchain-community/tree/main/libs/community) but we'll stick to a couple just so we can observe the cyclic nature of LangGraph in action!\n",
        "\n",
        "We'll leverage:\n",
        "\n",
        "- [Tavily Search Results](https://github.com/langchain-ai/langchain-community/blob/main/libs/community/langchain_community/tools/tavily_search/tool.py)\n",
        "- [Arxiv](https://github.com/langchain-ai/langchain-community/blob/main/libs/community/langchain_community/tools/arxiv/tool.py)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2k6n_Dob2F46"
      },
      "source": [
        "#### üèóÔ∏è Activity #1:\n",
        "\n",
        "Please add the tools to use into our toolbelt.\n",
        "\n",
        "> NOTE: Each tool in our toolbelt should be a method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "lAxaSvlfIeOg"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/_j/8hq__0h568j4qyjb3xwtnjzr0000gn/T/ipykernel_1829/1203815797.py:4: LangChainDeprecationWarning: The class `TavilySearchResults` was deprecated in LangChain 0.3.25 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-tavily package and should be used instead. To use it run `pip install -U :class:`~langchain-tavily` and import as `from :class:`~langchain_tavily import TavilySearch``.\n",
            "  tavily_tool = TavilySearchResults(max_results=5)\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "from langchain_community.tools.arxiv.tool import ArxivQueryRun\n",
        "\n",
        "tavily_tool = TavilySearchResults(max_results=5)\n",
        "\n",
        "tool_belt = [\n",
        "    tavily_tool,\n",
        "    ArxivQueryRun(),\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VI-C669ZYVI5"
      },
      "source": [
        "### Model\n",
        "\n",
        "Now we can set-up our model! We'll leverage the familiar OpenAI model suite for this example - but it's not *necessary* to use with LangGraph. LangGraph supports all models - though you might not find success with smaller models - as such, they recommend you stick with:\n",
        "\n",
        "- OpenAI's GPT-3.5 and GPT-4\n",
        "- Anthropic's Claude\n",
        "- Google's Gemini\n",
        "\n",
        "> NOTE: Because we're leveraging the OpenAI function calling API - we'll need to use OpenAI *for this specific example* (or any other service that exposes an OpenAI-style function calling API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "QkNS8rNZJs4z"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI(model=\"gpt-4.1-nano\", temperature=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ugkj3GzuZpQv"
      },
      "source": [
        "Now that we have our model set-up, let's \"put on the tool belt\", which is to say: We'll bind our LangChain formatted tools to the model in an OpenAI function calling format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "4OdMqFafZ_0V"
      },
      "outputs": [],
      "source": [
        "model = model.bind_tools(tool_belt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERzuGo6W18Lr"
      },
      "source": [
        "<div style=\"background-color: #204B8E; color: white; padding: 10px; border-radius: 5px;\">\n",
        "\n",
        "#### ‚ùì Question #1:\n",
        "\n",
        "How does the model determine which tool to use?\n",
        "\n",
        "It refers to the description of the tool to allow the model to decide if it is an appropriate tool to use.\n",
        "\n",
        "</di>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_296Ub96Z_H8"
      },
      "source": [
        "## Task 4: Putting the State in Stateful\n",
        "\n",
        "Earlier we used this phrasing:\n",
        "\n",
        "`coordinated multi-actor and stateful applications`\n",
        "\n",
        "So what does that \"stateful\" mean?\n",
        "\n",
        "To put it simply - we want to have some kind of object which we can pass around our application that holds information about what the current situation (state) is. Since our system will be constructed of many parts moving in a coordinated fashion - we want to be able to ensure we have some commonly understood idea of that state.\n",
        "\n",
        "LangGraph leverages a `StatefulGraph` which uses an `AgentState` object to pass information between the various nodes of the graph.\n",
        "\n",
        "There are more options than what we'll see below - but this `AgentState` object is one that is stored in a `TypedDict` with the key `messages` and the value is a `Sequence` of `BaseMessages` that will be appended to whenever the state changes.\n",
        "\n",
        "Let's think about a simple example to help understand exactly what this means (we'll simplify a great deal to try and clearly communicate what state is doing):\n",
        "\n",
        "1. We initialize our state object:\n",
        "  - `{\"messages\" : []}`\n",
        "2. Our user submits a query to our application.\n",
        "  - New State: `HumanMessage(#1)`\n",
        "  - `{\"messages\" : [HumanMessage(#1)}`\n",
        "3. We pass our state object to an Agent node which is able to read the current state. It will use the last `HumanMessage` as input. It gets some kind of output which it will add to the state.\n",
        "  - New State: `AgentMessage(#1, additional_kwargs {\"function_call\" : \"WebSearchTool\"})`\n",
        "  - `{\"messages\" : [HumanMessage(#1), AgentMessage(#1, ...)]}`\n",
        "4. We pass our state object to a \"conditional node\" (more on this later) which reads the last state to determine if we need to use a tool - which it can determine properly because of our provided object!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "mxL9b_NZKUdL"
      },
      "outputs": [],
      "source": [
        "from typing import TypedDict, Annotated\n",
        "from langgraph.graph.message import add_messages\n",
        "import operator\n",
        "from langchain_core.messages import BaseMessage\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "  messages: Annotated[list, add_messages]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWsMhfO9grLu"
      },
      "source": [
        "## Task 5: It's Graphing Time!\n",
        "\n",
        "Now that we have state, and we have tools, and we have an LLM - we can finally start making our graph!\n",
        "\n",
        "Let's take a second to refresh ourselves about what a graph is in this context.\n",
        "\n",
        "Graphs, also called networks in some circles, are a collection of connected objects.\n",
        "\n",
        "The objects in question are typically called nodes, or vertices, and the connections are called edges.\n",
        "\n",
        "Let's look at a simple graph.\n",
        "\n",
        "![image](https://i.imgur.com/2NFLnIc.png)\n",
        "\n",
        "Here, we're using the coloured circles to represent the nodes and the yellow lines to represent the edges. In this case, we're looking at a fully connected graph - where each node is connected by an edge to each other node.\n",
        "\n",
        "If we were to think about nodes in the context of LangGraph - we would think of a function, or an LCEL runnable.\n",
        "\n",
        "If we were to think about edges in the context of LangGraph - we might think of them as \"paths to take\" or \"where to pass our state object next\".\n",
        "\n",
        "Let's create some nodes and expand on our diagram.\n",
        "\n",
        "> NOTE: Due to the tight integration with LCEL - we can comfortably create our nodes in an async fashion!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "91flJWtZLUrl"
      },
      "outputs": [],
      "source": [
        "from langgraph.prebuilt import ToolNode\n",
        "\n",
        "def call_model(state):\n",
        "  messages = state[\"messages\"]\n",
        "  response = model.invoke(messages)\n",
        "  return {\"messages\" : [response]}\n",
        "\n",
        "tool_node = ToolNode(tool_belt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bwR7MgWj3Wg"
      },
      "source": [
        "Now we have two total nodes. We have:\n",
        "\n",
        "- `call_model` is a node that will...well...call the model\n",
        "- `tool_node` is a node which can call a tool\n",
        "\n",
        "Let's start adding nodes! We'll update our diagram along the way to keep track of what this looks like!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vF4_lgtmQNo",
        "outputId": "a4384377-8f7a-415f-be1b-fee6169cb101"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x11ba6ae40>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "uncompiled_graph = StateGraph(AgentState)\n",
        "\n",
        "uncompiled_graph.add_node(\"agent\", call_model)\n",
        "uncompiled_graph.add_node(\"action\", tool_node)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8CjRlbVmRpW"
      },
      "source": [
        "Let's look at what we have so far:\n",
        "\n",
        "![image](https://i.imgur.com/md7inqG.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaXHpPeSnOWC"
      },
      "source": [
        "Next, we'll add our entrypoint. All our entrypoint does is indicate which node is called first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGCbaYqRnmiw",
        "outputId": "5351807c-2ac7-4316-a3a3-878abeacd114"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x11ba6ae40>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "uncompiled_graph.set_entry_point(\"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUsfGoSpoF9U"
      },
      "source": [
        "![image](https://i.imgur.com/wNixpJe.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Q_pQgHmoW0M"
      },
      "source": [
        "Now we want to build a \"conditional edge\" which will use the output state of a node to determine which path to follow.\n",
        "\n",
        "We can help conceptualize this by thinking of our conditional edge as a conditional in a flowchart!\n",
        "\n",
        "Notice how our function simply checks if there is a \"function_call\" kwarg present.\n",
        "\n",
        "Then we create an edge where the origin node is our agent node and our destination node is *either* the action node or the END (finish the graph).\n",
        "\n",
        "It's important to highlight that the dictionary passed in as the third parameter (the mapping) should be created with the possible outputs of our conditional function in mind. In this case `should_continue` outputs either `\"end\"` or `\"continue\"` which are subsequently mapped to the action node or the END node."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BZgb81VQf9o",
        "outputId": "73a07c15-5f0b-40f2-b033-38b57d056dd8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x11ba6ae40>"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def should_continue(state):\n",
        "  last_message = state[\"messages\"][-1]\n",
        "\n",
        "  if last_message.tool_calls:\n",
        "    return \"action\"\n",
        "\n",
        "  return END\n",
        "\n",
        "uncompiled_graph.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    should_continue\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Cvhcf4jp0Ce"
      },
      "source": [
        "Let's visualize what this looks like.\n",
        "\n",
        "![image](https://i.imgur.com/8ZNwKI5.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKCjWJCkrJb9"
      },
      "source": [
        "Finally, we can add our last edge which will connect our action node to our agent node. This is because we *always* want our action node (which is used to call our tools) to return its output to our agent!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UvcgbHf1rIXZ",
        "outputId": "45d4bdd6-d6bb-4a1d-bb79-cad43c130bf2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x11ba6ae40>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "uncompiled_graph.add_edge(\"action\", \"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiWDwBQtrw7Z"
      },
      "source": [
        "Let's look at the final visualization.\n",
        "\n",
        "![image](https://i.imgur.com/NWO7usO.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYqDpErlsCsu"
      },
      "source": [
        "All that's left to do now is to compile our workflow - and we're off!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "zt9-KS8DpzNx"
      },
      "outputs": [],
      "source": [
        "simple_agent_graph = uncompiled_graph.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhNWIwBL1W4Q"
      },
      "source": [
        "<div style=\"background-color: #204B8E; color: white; padding: 10px; border-radius: 5px;\">\n",
        "\n",
        "#### ‚ùì Question #2:\n",
        "\n",
        "Is there any specific limit to how many times we can cycle?\n",
        "\n",
        "If not, how could we impose a limit to the number of cycles?\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEYcTShCsPaa"
      },
      "source": [
        "## Using Our Graph\n",
        "\n",
        "Now that we've created and compiled our graph - we can call it *just as we'd call any other* `Runnable`!\n",
        "\n",
        "Let's try out a few examples to see how it fairs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qn4n37PQRPII",
        "outputId": "5eeedfae-089d-496e-e71f-071939fa5832"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_lTshwh5Aswxi5NBE91rQEQNK', 'function': {'arguments': '{\"query\":\"current captain of the Winnipeg Jets\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 162, 'total_tokens': 185, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_438cf0ae7a', 'id': 'chatcmpl-CYru2NV1zCakoJiZxpV4hFJL4xAky', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--f0734991-2757-4eb9-b5eb-a9d4bd4f3ec5-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'current captain of the Winnipeg Jets'}, 'id': 'call_lTshwh5Aswxi5NBE91rQEQNK', 'type': 'tool_call'}], usage_metadata={'input_tokens': 162, 'output_tokens': 23, 'total_tokens': 185, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'action'\n",
            "[ToolMessage(content='[{\"title\": \"Adam Lowry - Wikipedia\", \"url\": \"https://en.wikipedia.org/wiki/Adam_Lowry\", \"content\": \"| Awards and achievements | | |\\\\n --- \\\\n| Preceded by Stefan Elliott | Winner of the Daryl K. (Doc) Seaman Trophy_Seaman_Trophy \\\\\"Daryl K. (Doc) Seaman Trophy\\\\\")  2010 | Succeeded by Colin Smith \\\\\"Colin Smith (ice hockey)\\\\\") |\\\\n| Sporting positions | | |\\\\n| Preceded by Blake Wheeler | Winnipeg Jets captain  2023‚Äìpresent | Incumbent | [...] Adam Lowry (born March 29, 1993) is a Canadian professional ice hockey centre \\\\\"Center (ice hockey)\\\\\") and  captain \\\\\"Captain (ice hockey)\\\\\") of the Winnipeg Jets of the National Hockey League (NHL).\\\\n\\\\n## Early life [...] Entering the 2023‚Äì24 season, Lowry was named captain of the Jets on September 12, 2023. He became the third in the team\\'s history since relocating to Winnipeg, and the tenth overall in franchise history.\\\\n\\\\nOn May 4, 2025, Lowry scored at 16:10 in double overtime to win 4-3 against the St. Louis Blues in Game 7 of the Jets-Blues first round playoff series.\\\\n\\\\n## Personal life\", \"score\": 0.8512743}, {\"title\": \"Winnipeg Jets - Wikipedia\", \"url\": \"https://en.wikipedia.org/wiki/Winnipeg_Jets\", \"content\": \"the draft lottery, which they used to select Finnish prospect Patrik Laine. Later that summer, the team appointed Blake Wheeler as their new captain. [...] In the 2021‚Äì22 season, the Jets finished a disappointing sixth in the Central Division, missing the playoffs. At the start of the 2022‚Äì23 season, forward Blake Wheeler was stripped of the team captaincy. The Jets then clinched the 2023 playoffs at the end of the regular season, but were defeated by the eventual Stanley Cup champion Vegas Golden Knights in five games in the first round. Before the start of the 2023‚Äì24 season, forward Adam Lowry was appointed team captain. The Jets then clinched [...] | 55 | Canada | Mark Scheifele (A#Alternate_captains \\\\\"Captain (ice hockey)\\\\\")) | C \\\\\"Centre (ice hockey)\\\\\") | R | 32 | 2011 | Kitchener, Ontario |\\\\n| 5 | Canada | Luke Schenn | D | R | 36 | 2025 | Saskatoon, Saskatchewan |\\\\n| 64 | Canada | Logan Stanley | D | L | 27 | 2016 | Kitchener, Ontario |\\\\n| 19 | Canada | Jonathan Toews | C \\\\\"Centre (ice hockey)\\\\\") | L | 37 | 2025 | Winnipeg, Manitoba |\\\\n| 13 | Canada | Gabriel Vilardi | C \\\\\"Centre (ice hockey)\\\\\") | R | 26 | 2023 | Kingston, Ontario |\", \"score\": 0.8319231}, {\"title\": \"Jets welcoming captain into lineup for season debut vs. Kings\", \"url\": \"https://clutchpoints.com/nhl/winnipeg-jets/jets-welcoming-captain-into-lineup-for-season-debut-vs-kings\", \"content\": \"The Winnipeg Jets have started their 2025-26 NHL campaign 9-3 and are just a point back of the top spot in league standings ‚Äî and that\\'s all been without the services of captain Adam Lowry. The veteran forward will rejoin the lineup for the first time this season on Tuesday night against the Kings in Los Angeles, the team announced. [...] ClutchPoints logo\\\\nfb-pixel\\\\nQuantcast\\\\ncomscore\\\\n\\\\n# Jets welcoming captain into lineup for season debut vs. Kings\\\\n\\\\nFor the first time this season, the Winnipeg Jets will have Adam Lowry in the lineup against the Los Angeles Kings on Tuesday.\\\\n\\\\nGoogle News Preferred Source\\\\nWinnipeg Jets center Adam Lowry (17) in action during the game between the Dallas Stars and the Winnipeg Jets in game three of the second round of the 2025 Stanley Cup Playoffs at American Airlines Center. [...] Adrian Kempe, Leo Carlsson, Leon Draisaitl and Nazem Kadri on one side, Sidney Crosby, Cole Caufield, Brad Marchand and Jake Guentzel on other side, NHL logo in middle, hockey rink in background\\\\nWinnipeg Jets center Jonathan Toews (19) celebrates after scoring a goal on Calgary Flames goaltender Dustin Wolf (32) during the third period at Scotiabank Saddledome.\\\\nWinnipeg Jets center Jonathan Toews (19) celebrates a goal against the Calgary Flames in the second period at Canada Life Centre.\", \"score\": 0.81025416}, {\"title\": \"Jets \\'getting closer\\' to contract extension with captain Lowry - TSN\", \"url\": \"https://www.tsn.ca/nhl/article/jets-getting-closer-to-contract-extension-with-captain-lowry/\", \"content\": \"TSN Hockey Insider Darren Dreger reports that the Jets and Lowry are getting close toa new deal, with his five-year, $16.25 million deal set to expire in July.\\\\n\\\\n‚ÄúYeah, it‚Äôs such a great boost for the Winnipeg Jets and the city of Winnipeg to get their captain, Adam Lowry, back in the mix. It‚Äôs also been a priority for a long, long time by the organization to get Lowry locked up longer term,‚Äú Dreger explained on Insider Trading. [...] The 32-year-old forward had 16 goals and 18 assists in 73 games last year with the Jets.\\\\n\\\\nSelected in the third round by Winnipeg in 2011, Lowry was named captain of the Jets ahead of the 2023-24 season. He helped the Jets capture the Presidents‚Äô Trophy last season after a 116-point regular season campaign.\\\\n\\\\nThe St. Louis native has scored 121 goals with 273 points in 776 career NHL games, all with Winnipeg. [...] comscore\\\\nTSN Logo\\\\n\\\\n# Jets ‚Äògetting closer‚Äô to contract extension with captain Lowry\\\\n\\\\nPublished: November 05, 2025 at 8:46AM EST\\\\n\\\\nInsider Trading: 10-game milestone looms for NHL rookies\\\\n\\\\n## Insider Trading: 10-game milestone looms for NHL rookies\\\\n\\\\nNHL: Jets 0, Kings 3\\\\n\\\\n## NHL: Jets 0, Kings 3\\\\n\\\\nWinnipeg Jets captain Adam Lowry made his season debut on Tuesday, suiting up with an expiring contract, but it appears ifan extensionmay not be far off.\", \"score\": 0.78512776}, {\"title\": \"Lowry to make season debut with Jets after hip surgery | NHL.com\", \"url\": \"https://www.nhl.com/news/adam-lowry-likely-back-from-surgery-for-winnipeg\", \"content\": \"NHL logo\\\\nNHL logo\\\\n\\\\n# Lowry to make season debut with Jets after hip surgery\\\\n\\\\nCaptain missed 1st 12 games, will likely center 3rd line against Kings\\\\n\\\\nLowry_WPG_skating\\\\n\\\\n¬©\\\\nGetty Images\\\\n\\\\nWINNIPEG -- Adam Lowry will make his season debut when the Winnipeg Jets begin a six-game road trip at the Los Angeles Kings on Tuesday (10:30 p.m. ET; FDSNW, TSN3). [...] Lights, camera, action üé¨  \\\\nAdam Lowry is set to make his season debut in LA.@JamieThomasTV BLOG üîΩ pic.twitter.com/9VIU5TTv4D\\\\n\\\\nLeadership, physicality and a 200-foot defensive game are the staples of what Lowry brings to the Jets. He also set NHL career highs in goals (16) and plus/minus (plus-18) for Winnipeg last season and had 18 assists and 34 points in 73 games.\", \"score\": 0.63750535}]', name='tavily_search_results_json', id='99adf95c-3dce-4971-94db-70b5ac6664c4', tool_call_id='call_lTshwh5Aswxi5NBE91rQEQNK', artifact={'query': 'current captain of the Winnipeg Jets', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://en.wikipedia.org/wiki/Adam_Lowry', 'title': 'Adam Lowry - Wikipedia', 'content': '| Awards and achievements | | |\\n --- \\n| Preceded by Stefan Elliott | Winner of the Daryl K. (Doc) Seaman Trophy_Seaman_Trophy \"Daryl K. (Doc) Seaman Trophy\")  2010 | Succeeded by Colin Smith \"Colin Smith (ice hockey)\") |\\n| Sporting positions | | |\\n| Preceded by Blake Wheeler | Winnipeg Jets captain  2023‚Äìpresent | Incumbent | [...] Adam Lowry (born March 29, 1993) is a Canadian professional ice hockey centre \"Center (ice hockey)\") and  captain \"Captain (ice hockey)\") of the Winnipeg Jets of the National Hockey League (NHL).\\n\\n## Early life [...] Entering the 2023‚Äì24 season, Lowry was named captain of the Jets on September 12, 2023. He became the third in the team\\'s history since relocating to Winnipeg, and the tenth overall in franchise history.\\n\\nOn May 4, 2025, Lowry scored at 16:10 in double overtime to win 4-3 against the St. Louis Blues in Game 7 of the Jets-Blues first round playoff series.\\n\\n## Personal life', 'score': 0.8512743, 'raw_content': None}, {'url': 'https://en.wikipedia.org/wiki/Winnipeg_Jets', 'title': 'Winnipeg Jets - Wikipedia', 'content': 'the draft lottery, which they used to select Finnish prospect Patrik Laine. Later that summer, the team appointed Blake Wheeler as their new captain. [...] In the 2021‚Äì22 season, the Jets finished a disappointing sixth in the Central Division, missing the playoffs. At the start of the 2022‚Äì23 season, forward Blake Wheeler was stripped of the team captaincy. The Jets then clinched the 2023 playoffs at the end of the regular season, but were defeated by the eventual Stanley Cup champion Vegas Golden Knights in five games in the first round. Before the start of the 2023‚Äì24 season, forward Adam Lowry was appointed team captain. The Jets then clinched [...] | 55 | Canada | Mark Scheifele (A#Alternate_captains \"Captain (ice hockey)\")) | C \"Centre (ice hockey)\") | R | 32 | 2011 | Kitchener, Ontario |\\n| 5 | Canada | Luke Schenn | D | R | 36 | 2025 | Saskatoon, Saskatchewan |\\n| 64 | Canada | Logan Stanley | D | L | 27 | 2016 | Kitchener, Ontario |\\n| 19 | Canada | Jonathan Toews | C \"Centre (ice hockey)\") | L | 37 | 2025 | Winnipeg, Manitoba |\\n| 13 | Canada | Gabriel Vilardi | C \"Centre (ice hockey)\") | R | 26 | 2023 | Kingston, Ontario |', 'score': 0.8319231, 'raw_content': None}, {'url': 'https://clutchpoints.com/nhl/winnipeg-jets/jets-welcoming-captain-into-lineup-for-season-debut-vs-kings', 'title': 'Jets welcoming captain into lineup for season debut vs. Kings', 'content': \"The Winnipeg Jets have started their 2025-26 NHL campaign 9-3 and are just a point back of the top spot in league standings ‚Äî and that's all been without the services of captain Adam Lowry. The veteran forward will rejoin the lineup for the first time this season on Tuesday night against the Kings in Los Angeles, the team announced. [...] ClutchPoints logo\\nfb-pixel\\nQuantcast\\ncomscore\\n\\n# Jets welcoming captain into lineup for season debut vs. Kings\\n\\nFor the first time this season, the Winnipeg Jets will have Adam Lowry in the lineup against the Los Angeles Kings on Tuesday.\\n\\nGoogle News Preferred Source\\nWinnipeg Jets center Adam Lowry (17) in action during the game between the Dallas Stars and the Winnipeg Jets in game three of the second round of the 2025 Stanley Cup Playoffs at American Airlines Center. [...] Adrian Kempe, Leo Carlsson, Leon Draisaitl and Nazem Kadri on one side, Sidney Crosby, Cole Caufield, Brad Marchand and Jake Guentzel on other side, NHL logo in middle, hockey rink in background\\nWinnipeg Jets center Jonathan Toews (19) celebrates after scoring a goal on Calgary Flames goaltender Dustin Wolf (32) during the third period at Scotiabank Saddledome.\\nWinnipeg Jets center Jonathan Toews (19) celebrates a goal against the Calgary Flames in the second period at Canada Life Centre.\", 'score': 0.81025416, 'raw_content': None}, {'url': 'https://www.tsn.ca/nhl/article/jets-getting-closer-to-contract-extension-with-captain-lowry/', 'title': \"Jets 'getting closer' to contract extension with captain Lowry - TSN\", 'content': 'TSN Hockey Insider Darren Dreger reports that the Jets and Lowry are getting close toa new deal, with his five-year, $16.25 million deal set to expire in July.\\n\\n‚ÄúYeah, it‚Äôs such a great boost for the Winnipeg Jets and the city of Winnipeg to get their captain, Adam Lowry, back in the mix. It‚Äôs also been a priority for a long, long time by the organization to get Lowry locked up longer term,‚Äú Dreger explained on Insider Trading. [...] The 32-year-old forward had 16 goals and 18 assists in 73 games last year with the Jets.\\n\\nSelected in the third round by Winnipeg in 2011, Lowry was named captain of the Jets ahead of the 2023-24 season. He helped the Jets capture the Presidents‚Äô Trophy last season after a 116-point regular season campaign.\\n\\nThe St. Louis native has scored 121 goals with 273 points in 776 career NHL games, all with Winnipeg. [...] comscore\\nTSN Logo\\n\\n# Jets ‚Äògetting closer‚Äô to contract extension with captain Lowry\\n\\nPublished: November 05, 2025 at 8:46AM EST\\n\\nInsider Trading: 10-game milestone looms for NHL rookies\\n\\n## Insider Trading: 10-game milestone looms for NHL rookies\\n\\nNHL: Jets 0, Kings 3\\n\\n## NHL: Jets 0, Kings 3\\n\\nWinnipeg Jets captain Adam Lowry made his season debut on Tuesday, suiting up with an expiring contract, but it appears ifan extensionmay not be far off.', 'score': 0.78512776, 'raw_content': None}, {'url': 'https://www.nhl.com/news/adam-lowry-likely-back-from-surgery-for-winnipeg', 'title': 'Lowry to make season debut with Jets after hip surgery | NHL.com', 'content': 'NHL logo\\nNHL logo\\n\\n# Lowry to make season debut with Jets after hip surgery\\n\\nCaptain missed 1st 12 games, will likely center 3rd line against Kings\\n\\nLowry_WPG_skating\\n\\n¬©\\nGetty Images\\n\\nWINNIPEG -- Adam Lowry will make his season debut when the Winnipeg Jets begin a six-game road trip at the Los Angeles Kings on Tuesday (10:30 p.m. ET; FDSNW, TSN3). [...] Lights, camera, action üé¨  \\nAdam Lowry is set to make his season debut in LA.@JamieThomasTV BLOG üîΩ pic.twitter.com/9VIU5TTv4D\\n\\nLeadership, physicality and a 200-foot defensive game are the staples of what Lowry brings to the Jets. He also set NHL career highs in goals (16) and plus/minus (plus-18) for Winnipeg last season and had 18 assists and 34 points in 73 games.', 'score': 0.63750535, 'raw_content': None}], 'response_time': 3.42, 'request_id': 'ba561827-355a-4644-a13e-7c0e4b0f27a7'})]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content='The current captain of the Winnipeg Jets is Adam Lowry.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 1890, 'total_tokens': 1903, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_438cf0ae7a', 'id': 'chatcmpl-CYru9KYYuJloSIXCpzdk7EEpKvIqa', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--b29b3021-91a4-4a9a-8d47-39b1b8789cc9-0', usage_metadata={'input_tokens': 1890, 'output_tokens': 13, 'total_tokens': 1903, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "inputs = {\"messages\" : [HumanMessage(content=\"Who is the current captain of the Winnipeg Jets?\")]}\n",
        "\n",
        "async for chunk in simple_agent_graph.astream(inputs, stream_mode=\"updates\"):\n",
        "    for node, values in chunk.items():\n",
        "        print(f\"Receiving update from node: '{node}'\")\n",
        "        print(values[\"messages\"])\n",
        "        print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBHnUtLSscRr"
      },
      "source": [
        "Let's look at what happened:\n",
        "\n",
        "1. Our state object was populated with our request\n",
        "2. The state object was passed into our entry point (agent node) and the agent node added an `AIMessage` to the state object and passed it along the conditional edge\n",
        "3. The conditional edge received the state object, found the \"tool_calls\" `additional_kwarg`, and sent the state object to the action node\n",
        "4. The action node added the response from the OpenAI function calling endpoint to the state object and passed it along the edge to the agent node\n",
        "5. The agent node added a response to the state object and passed it along the conditional edge\n",
        "6. The conditional edge received the state object, could not find the \"tool_calls\" `additional_kwarg` and passed the state object to END where we see it output in the cell above!\n",
        "\n",
        "Now let's look at an example that shows a multiple tool usage - all with the same flow!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afv2BuEsV5JG",
        "outputId": "ff009536-d281-4a56-c126-9cd245352bfe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_Jruw4ukfbEeCnVd4fc2LAfYf', 'function': {'arguments': '{\"query\": \"QLoRA\"}', 'name': 'arxiv'}, 'type': 'function'}, {'id': 'call_MrefciwxUFGOrYacdtk10JPY', 'function': {'arguments': '{\"query\": \"latest Tweet of author\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 54, 'prompt_tokens': 178, 'total_tokens': 232, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_438cf0ae7a', 'id': 'chatcmpl-CYrwHqLfdjAJ16ZJ9WmVRcvKAbVsV', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--b6e27044-acd2-491f-b564-6bd2576093f0-0', tool_calls=[{'name': 'arxiv', 'args': {'query': 'QLoRA'}, 'id': 'call_Jruw4ukfbEeCnVd4fc2LAfYf', 'type': 'tool_call'}, {'name': 'tavily_search_results_json', 'args': {'query': 'latest Tweet of author'}, 'id': 'call_MrefciwxUFGOrYacdtk10JPY', 'type': 'tool_call'}], usage_metadata={'input_tokens': 178, 'output_tokens': 54, 'total_tokens': 232, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'action'\n",
            "Tool Used: arxiv\n",
            "[ToolMessage(content='Published: 2023-05-23\\nTitle: QLoRA: Efficient Finetuning of Quantized LLMs\\nAuthors: Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer\\nSummary: We present QLoRA, an efficient finetuning approach that reduces memory usage\\nenough to finetune a 65B parameter model on a single 48GB GPU while preserving\\nfull 16-bit finetuning task performance. QLoRA backpropagates gradients through\\na frozen, 4-bit quantized pretrained language model into Low Rank\\nAdapters~(LoRA). Our best model family, which we name Guanaco, outperforms all\\nprevious openly released models on the Vicuna benchmark, reaching 99.3% of the\\nperformance level of ChatGPT while only requiring 24 hours of finetuning on a\\nsingle GPU. QLoRA introduces a number of innovations to save memory without\\nsacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is\\ninformation theoretically optimal for normally distributed weights (b) double\\nquantization to reduce the average memory footprint by quantizing the\\nquantization constants, and (c) paged optimziers to manage memory spikes. We\\nuse QLoRA to finetune more than 1,000 models, providing a detailed analysis of\\ninstruction following and chatbot performance across 8 instruction datasets,\\nmultiple model types (LLaMA, T5), and model scales that would be infeasible to\\nrun with regular finetuning (e.g. 33B and 65B parameter models). Our results\\nshow that QLoRA finetuning on a small high-quality dataset leads to\\nstate-of-the-art results, even when using smaller models than the previous\\nSoTA. We provide a detailed analysis of chatbot performance based on both human\\nand GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable\\nalternative to human evaluation. Furthermore, we find that current chatbot\\nbenchmarks are not trustworthy to accurately evaluate the performance levels of\\nchatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to\\nChatGPT. We release all of our models and code, including CUDA kernels for\\n4-bit training.\\n\\nPublished: 2024-05-27\\nTitle: Accurate LoRA-Finetuning Quantization of LLMs via Information Retention\\nAuthors: Haotong Qin, Xudong Ma, Xingyu Zheng, Xiaoyang Li, Yang Zhang, Shouda Liu, Jie Luo, Xianglong Liu, Michele Magno\\nSummary: The LoRA-finetuning quantization of LLMs has been extensively studied to\\nobtain accurate yet compact LLMs for deployment on resource-constrained\\nhardware. However, existing methods cause the quantized LLM to severely degrade\\nand even fail to benefit from the finetuning of LoRA. This paper proposes a\\nnovel IR-QLoRA for pushing quantized LLMs with LoRA to be highly accurate\\nthrough information retention. The proposed IR-QLoRA mainly relies on two\\ntechnologies derived from the perspective of unified information: (1)\\nstatistics-based Information Calibration Quantization allows the quantized\\nparameters of LLM to retain original information accurately; (2)\\nfinetuning-based Information Elastic Connection makes LoRA utilizes elastic\\nrepresentation transformation with diverse information. Comprehensive\\nexperiments show that IR-QLoRA can significantly improve accuracy across LLaMA\\nand LLaMA2 families under 2-4 bit-widths, e.g., 4- bit LLaMA-7B achieves 1.4%\\nimprovement on MMLU compared with the state-of-the-art methods. The significant\\nperformance gain requires only a tiny 0.31% additional time consumption,\\nrevealing the satisfactory efficiency of our IR-QLoRA. We highlight that\\nIR-QLoRA enjoys excellent versatility, compatible with various frameworks\\n(e.g., NormalFloat and Integer quantization) and brings general accuracy gains.\\nThe code is available at https://github.com/htqin/ir-qlora.\\n\\nPublished: 2025-02-05\\nTitle: Resource-Efficient & Effective Code Summarization\\nAuthors: Saima Afrin, Joseph Call, Khai-Nguyen Nguyen, Oscar Chaparro, Antonio Mastropaolo\\nSummary: Code Language Models (CLMs) have demonstrated high effectiveness in\\nautomating software engineering tasks such as bug fixing, code generation, and\\ncode documentation. This ', name='arxiv', id='f43b02fa-b87b-4f38-a5df-4367edb46b08', tool_call_id='call_Jruw4ukfbEeCnVd4fc2LAfYf'), ToolMessage(content='[{\"title\": \"A Complete Breakdown of the J.K. Rowling Transgender-Comments ...\", \"url\": \"https://www.glamour.com/story/a-complete-breakdown-of-the-jk-rowling-transgender-comments-controversy\", \"content\": \"## Rowling mocks TV show boycotters.\\\\n\\\\nWith news of the HBO Max reboot, all eyes are on J.K. Rowling‚Äîand she keeps giving her critics something to talk about. The author responded to fans‚Äô calls to boycott the show on Twitter on April 21, 2023: ‚ÄúDreadful news, which I feel duty bound to share,‚Äù she wrote. ‚ÄúActivists in my mentions are trying to organize yet another boycott of my work, this time of the Harry Potter TV show.‚Äù The sarcasm speaks volumes.\\\\n\\\\n#### X content [...] She once again seemingly called into question the use of hormones. ‚ÄúThe long-term health risks of cross-sex hormones have been now been tracked over a lengthy period,‚Äù she tweeted. ‚ÄúThese side-effects are often minimised or denied by trans activists‚Ä¶. None of that may trouble you or disturb your belief in your own righteousness. But if so, I can‚Äôt pretend I care much about your bad opinion of me.‚Äù\\\\n\\\\n#### X content\\\\n\\\\n#### X content\\\\n\\\\n#### X content\\\\n\\\\n## Her latest books aren‚Äôt helping: Part one. [...] Khelif, who is not trans, filed a lawsuit against Rowling and X owner Elon Musk for the ‚Äúcyberbullying‚Äù she received as a result of their tirades against her. Instead of becoming more cautious, this apparently caused Rowling to pivot from speculating about athletes to simply finding and mocking the trans ones. In an especially cruel move, she took to Twitter in September 2024 to pick at an Italian Paralympic athlete who is visually impaired and, yes, a trans woman.\", \"score\": 0.5053471}, {\"title\": \"Top 50 Book Influencers on Twitter in 2025\", \"url\": \"https://x.feedspot.com/book_twitter_influencers/\", \"content\": \"Ova Ceren Bio Half woman, half book. Immigrant. Writer. THE BOOK OF HEARTBREAK August 2025 @hotkeybooks @alcovepress Rep: @ejcounsell @Northbanktalent \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\@ovaceren.comTwitter Handle   @excusemereading Twitter Followers  8.8K Type  Nano Gender  Female   Get Email Contact\\\\n\\\\nOva Ceren\\\\n\\\\n### 48. Cait Jacobs [...] Andrew Joseph White  Bio #1 bestselling author, trans autistic monster boy ‚Ä¢ HELL FOLLOWED WITH US ‚Ä¢ up next: YOU WEREN‚ÄôT MEANT TO BE HUMAN (Sept 9) ‚Ä¢ rep @marchsoloway ‚Ä¢ he/him \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\@gmail.comTwitter Handle   @AJWhiteAuthor Twitter Followers  14.3K Type  Micro Gender  Male   Get Email Contact\\\\n\\\\nAndrew Joseph White \\\\n\\\\n### 42. Joshua Gamon [...] Kendare Blake Bio #1 NYT Bestselling author of Three Dark Crowns, Anna Dressed in Blood, and more. Avid reader. King of Snakes.NEXT: ‚öîÔ∏èCHAMPION OF FATE‚öîÔ∏è September 19, 2023 \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\@yahoo.comTwitter Handle   @kendareblake Twitter Followers  15K Type  Micro Gender  Female   Get Email Contact\\\\n\\\\nKendare Blake\\\\n\\\\n### 40. Olivie Blake\", \"score\": 0.30182534}, {\"title\": \"My Internet: Mary H.K. Choi - by Nick Catucci - Embedded\", \"url\": \"https://embedded.substack.com/p/my-internet-mary-hk-choi\", \"content\": \"Mary finds hyperventilating women telling her to buy jelly bras on TikTok soothing, misses ‚Äúweird pancake syrup smell on the West side of New York‚Äù Twitter, and is grateful for Snapchat because when DJ Khaled got really big on there she was hired to ghostwrite his book. ‚ÄîNick [...] `MARY H.K. CHOI:` Yeah, it was on Twitter. ‚ÄúCan someone fix bacon packaging, this has gone on long enough.‚Äù\\\\n\\\\n`MARY H.K. CHOI:` [...] `MARY H.K. CHOI:` I love the way Gen Z over-explains everything to each other. Like, how amazing lemon juice is on a salad or that you can actually call a restaurant and have food delivered as a hack so you don‚Äôt have to pay app surcharges. It‚Äôs like I can get mad that the ‚Äúviral‚Äù Kendall Jenner tank top is literally a boy‚Äôs undershirt from Hanes or think it‚Äôs pure and astonishing and weirdly cool.\\\\n\\\\n`MARY H.K. CHOI:`\", \"score\": 0.16410108}, {\"title\": \"How to choose the PERFECT author pen name: and when you should!\", \"url\": \"https://www.creativindie.com/how-to-create-an-author-pen-name-and-when-you-should/\", \"content\": \"derekmurphy23\\\\nderekmurphy23\\\\n\\\\nI‚Äôm in a decades-long quest to make money online with my art and writing‚Ä¶ so I can create fun stuff all day. I did spend 10 years getting a PhD so sometimes I can sound smart; but most often not. I covet a castle full of cats, where I can write and eat cake. Stick around for publishing & marketing strategies for modern creatives!\\\\n\\\\n### Sharing is caring!\\\\n\\\\n### Related\\\\n\\\\n### 5 Comments [...] ### Add Comment\\\\n\\\\n### Cancel reply\\\\n\\\\nYour email address will not be published. Required fields are marked \\\\\\\\\\\\n\\\\nSave my name, email, and website in this browser for the next time I comment.\\\\n\\\\nŒî\\\\n\\\\n##### Tags\\\\n\\\\nüóùÔ∏è\\\\n\\\\nA 6-Step Plan to Unlock\\\\n\\\\n##### Your Best Work\\\\n\\\\n##### \\\\n\\\\nwriting tips for authors\\\\nwriting tips for authors\\\\n\\\\n##### \\\\n\\\\nTip Jar (donations appreciated!)\\\\\")\\\\nTip Jar (donations appreciated!)\\\\\")\\\\nLogo\\\\n\\\\nDerek Murphy ¬© 2025. All Rights Reserved [...] Go from zero platform to #1 bestseller in 90 days or less with my book, Guerrilla Publishing. Download now for free and get access to my new companion workbook and book launch roadmap (this is advanced stuff you won‚Äôt find anywhere else). Download the book launch checklist\\\\n\\\\nwriting tips, self-publishing and book marketing for authors\\\\nwriting tips, self-publishing and book marketing for authors\\\\n\\\\nThe 3 secrets to book marketing, and a haunted castle tour.\\\\n\\\\nTotally free. Get it here.\", \"score\": 0.084954865}, {\"title\": \"Research Impact : Establishing Your Author Name and Presence\", \"url\": \"https://beckerguides.wustl.edu/c.php?g=299587&p=2001207\", \"content\": \"\\\\\\\\As of March 2025, NIH has postponed the May 25, 2025 deadline for use of SciENcv Common forms to complete  Biographical Sketch and Current and Pending (Other) Support documents. A new date has not yet been announced. See Common Forms for Biographical Sketch and Current and Pending (Other) Support.\\\\n\\\\n. This means that ORCID is not limited to a specific platform and is a non-proprietary means of establishing your author name. [...] We highly recommend that authors create an ORCID ID, and check their profiles in Scopus, Web of Science and Google Scholar. An ORCID ID can be created using publication data from Scopus or Web of Science/Researcher ID. Authors may also want to consider creating a LinkedIn or Doximity profile. WashU Medicine physicians are highly encouraged to claim their Doximity profiles.\\\\n\\\\n## LinkedIn\", \"score\": 0.03248222}]', name='tavily_search_results_json', id='ad65a09d-3e00-4341-9e3d-26147290faa5', tool_call_id='call_MrefciwxUFGOrYacdtk10JPY', artifact={'query': 'latest Tweet of author', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://www.glamour.com/story/a-complete-breakdown-of-the-jk-rowling-transgender-comments-controversy', 'title': 'A Complete Breakdown of the J.K. Rowling Transgender-Comments ...', 'content': '## Rowling mocks TV show boycotters.\\n\\nWith news of the HBO Max reboot, all eyes are on J.K. Rowling‚Äîand she keeps giving her critics something to talk about. The author responded to fans‚Äô calls to boycott the show on Twitter on April 21, 2023: ‚ÄúDreadful news, which I feel duty bound to share,‚Äù she wrote. ‚ÄúActivists in my mentions are trying to organize yet another boycott of my work, this time of the Harry Potter TV show.‚Äù The sarcasm speaks volumes.\\n\\n#### X content [...] She once again seemingly called into question the use of hormones. ‚ÄúThe long-term health risks of cross-sex hormones have been now been tracked over a lengthy period,‚Äù she tweeted. ‚ÄúThese side-effects are often minimised or denied by trans activists‚Ä¶. None of that may trouble you or disturb your belief in your own righteousness. But if so, I can‚Äôt pretend I care much about your bad opinion of me.‚Äù\\n\\n#### X content\\n\\n#### X content\\n\\n#### X content\\n\\n## Her latest books aren‚Äôt helping: Part one. [...] Khelif, who is not trans, filed a lawsuit against Rowling and X owner Elon Musk for the ‚Äúcyberbullying‚Äù she received as a result of their tirades against her. Instead of becoming more cautious, this apparently caused Rowling to pivot from speculating about athletes to simply finding and mocking the trans ones. In an especially cruel move, she took to Twitter in September 2024 to pick at an Italian Paralympic athlete who is visually impaired and, yes, a trans woman.', 'score': 0.5053471, 'raw_content': None}, {'url': 'https://x.feedspot.com/book_twitter_influencers/', 'title': 'Top 50 Book Influencers on Twitter in 2025', 'content': 'Ova Ceren Bio Half woman, half book. Immigrant. Writer. THE BOOK OF HEARTBREAK August 2025 @hotkeybooks @alcovepress Rep: @ejcounsell @Northbanktalent \\\\\\\\\\\\\\\\@ovaceren.comTwitter Handle   @excusemereading Twitter Followers  8.8K Type  Nano Gender  Female   Get Email Contact\\n\\nOva Ceren\\n\\n### 48. Cait Jacobs [...] Andrew Joseph White  Bio #1 bestselling author, trans autistic monster boy ‚Ä¢ HELL FOLLOWED WITH US ‚Ä¢ up next: YOU WEREN‚ÄôT MEANT TO BE HUMAN (Sept 9) ‚Ä¢ rep @marchsoloway ‚Ä¢ he/him \\\\\\\\\\\\\\\\@gmail.comTwitter Handle   @AJWhiteAuthor Twitter Followers  14.3K Type  Micro Gender  Male   Get Email Contact\\n\\nAndrew Joseph White \\n\\n### 42. Joshua Gamon [...] Kendare Blake Bio #1 NYT Bestselling author of Three Dark Crowns, Anna Dressed in Blood, and more. Avid reader. King of Snakes.NEXT: ‚öîÔ∏èCHAMPION OF FATE‚öîÔ∏è September 19, 2023 \\\\\\\\\\\\\\\\@yahoo.comTwitter Handle   @kendareblake Twitter Followers  15K Type  Micro Gender  Female   Get Email Contact\\n\\nKendare Blake\\n\\n### 40. Olivie Blake', 'score': 0.30182534, 'raw_content': None}, {'url': 'https://embedded.substack.com/p/my-internet-mary-hk-choi', 'title': 'My Internet: Mary H.K. Choi - by Nick Catucci - Embedded', 'content': 'Mary finds hyperventilating women telling her to buy jelly bras on TikTok soothing, misses ‚Äúweird pancake syrup smell on the West side of New York‚Äù Twitter, and is grateful for Snapchat because when DJ Khaled got really big on there she was hired to ghostwrite his book. ‚ÄîNick [...] `MARY H.K. CHOI:` Yeah, it was on Twitter. ‚ÄúCan someone fix bacon packaging, this has gone on long enough.‚Äù\\n\\n`MARY H.K. CHOI:` [...] `MARY H.K. CHOI:` I love the way Gen Z over-explains everything to each other. Like, how amazing lemon juice is on a salad or that you can actually call a restaurant and have food delivered as a hack so you don‚Äôt have to pay app surcharges. It‚Äôs like I can get mad that the ‚Äúviral‚Äù Kendall Jenner tank top is literally a boy‚Äôs undershirt from Hanes or think it‚Äôs pure and astonishing and weirdly cool.\\n\\n`MARY H.K. CHOI:`', 'score': 0.16410108, 'raw_content': None}, {'url': 'https://www.creativindie.com/how-to-create-an-author-pen-name-and-when-you-should/', 'title': 'How to choose the PERFECT author pen name: and when you should!', 'content': 'derekmurphy23\\nderekmurphy23\\n\\nI‚Äôm in a decades-long quest to make money online with my art and writing‚Ä¶ so I can create fun stuff all day. I did spend 10 years getting a PhD so sometimes I can sound smart; but most often not. I covet a castle full of cats, where I can write and eat cake. Stick around for publishing & marketing strategies for modern creatives!\\n\\n### Sharing is caring!\\n\\n### Related\\n\\n### 5 Comments [...] ### Add Comment\\n\\n### Cancel reply\\n\\nYour email address will not be published. Required fields are marked \\\\\\n\\nSave my name, email, and website in this browser for the next time I comment.\\n\\nŒî\\n\\n##### Tags\\n\\nüóùÔ∏è\\n\\nA 6-Step Plan to Unlock\\n\\n##### Your Best Work\\n\\n##### \\n\\nwriting tips for authors\\nwriting tips for authors\\n\\n##### \\n\\nTip Jar (donations appreciated!)\")\\nTip Jar (donations appreciated!)\")\\nLogo\\n\\nDerek Murphy ¬© 2025. All Rights Reserved [...] Go from zero platform to #1 bestseller in 90 days or less with my book, Guerrilla Publishing. Download now for free and get access to my new companion workbook and book launch roadmap (this is advanced stuff you won‚Äôt find anywhere else). Download the book launch checklist\\n\\nwriting tips, self-publishing and book marketing for authors\\nwriting tips, self-publishing and book marketing for authors\\n\\nThe 3 secrets to book marketing, and a haunted castle tour.\\n\\nTotally free. Get it here.', 'score': 0.084954865, 'raw_content': None}, {'url': 'https://beckerguides.wustl.edu/c.php?g=299587&p=2001207', 'title': 'Research Impact : Establishing Your Author Name and Presence', 'content': '\\\\As of March 2025, NIH has postponed the May 25, 2025 deadline for use of SciENcv Common forms to complete  Biographical Sketch and Current and Pending (Other) Support documents. A new date has not yet been announced. See Common Forms for Biographical Sketch and Current and Pending (Other) Support.\\n\\n. This means that ORCID is not limited to a specific platform and is a non-proprietary means of establishing your author name. [...] We highly recommend that authors create an ORCID ID, and check their profiles in Scopus, Web of Science and Google Scholar. An ORCID ID can be created using publication data from Scopus or Web of Science/Researcher ID. Authors may also want to consider creating a LinkedIn or Doximity profile. WashU Medicine physicians are highly encouraged to claim their Doximity profiles.\\n\\n## LinkedIn', 'score': 0.03248222, 'raw_content': None}], 'response_time': 3.2, 'request_id': '151a186d-81a0-4792-b80c-1e914ae2fb85'})]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content='I found the QLoRA paper on Arxiv, titled \"QLoRA: Efficient Finetuning of Quantized LLMs,\" authored by Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. \\n\\nRegarding the authors\\' latest Tweets:\\n- The search results included various recent tweets, but specific latest tweets for each author were not directly retrieved in detail. Would you like me to perform a more targeted search for each individual author to find their latest tweets?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 106, 'prompt_tokens': 2967, 'total_tokens': 3073, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_438cf0ae7a', 'id': 'chatcmpl-CYrwNH2F4hXdESzTQ75Gg9DfvltsO', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--6564ed8d-e2f4-4e44-8d2b-f4229d2aae38-0', usage_metadata={'input_tokens': 2967, 'output_tokens': 106, 'total_tokens': 3073, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "inputs = {\"messages\" : [HumanMessage(content=\"Search Arxiv for the QLoRA paper, then search each of the authors to find out their latest Tweet using Tavily!\")]}\n",
        "\n",
        "async for chunk in simple_agent_graph.astream(inputs, stream_mode=\"updates\"):\n",
        "    for node, values in chunk.items():\n",
        "        print(f\"Receiving update from node: '{node}'\")\n",
        "        if node == \"action\":\n",
        "          print(f\"Tool Used: {values['messages'][0].name}\")\n",
        "        print(values[\"messages\"])\n",
        "\n",
        "        print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXzDlZVz1Hnf"
      },
      "source": [
        "#### üèóÔ∏è Activity #2:\n",
        "\n",
        "Please write out the steps the agent took to arrive at the correct answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7c8-Uyarh1v"
      },
      "source": [
        "# LangSmith Evaluator (Optional)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pV3XeFOT1Sar"
      },
      "source": [
        "### Pre-processing for LangSmith"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wruQCuzewUuO"
      },
      "source": [
        "To do a little bit more preprocessing, let's wrap our LangGraph agent in a simple chain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "oeXdQgbxwhTv"
      },
      "outputs": [],
      "source": [
        "def convert_inputs(input_object):\n",
        "  return {\"messages\" : [HumanMessage(content=input_object[\"question\"])]}\n",
        "\n",
        "def parse_output(input_state):\n",
        "  return input_state[\"messages\"][-1].content\n",
        "\n",
        "agent_chain_with_formatting = convert_inputs | simple_agent_graph | parse_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "orYxBZXSxJjZ",
        "outputId": "76be837b-6424-4516-8f63-07fbd8c25bf5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"RAG can refer to different concepts depending on the context. Could you please specify whether you're asking about RAG in the context of project management, machine learning, or another field?\""
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "agent_chain_with_formatting.invoke({\"question\" : \"What is RAG?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9UkCIqkpyZu"
      },
      "source": [
        "### Task 1: Creating An Evaluation Dataset\n",
        "\n",
        "Just as we saw last week, we'll want to create a dataset to test our Agent's ability to answer questions.\n",
        "\n",
        "In order to do this - we'll want to provide some questions and some answers. Let's look at how we can create such a dataset below.\n",
        "\n",
        "```python\n",
        "questions = [\n",
        "    \"What optimizer is used in QLoRA?\",\n",
        "    \"What data type was created in the QLoRA paper?\",\n",
        "    \"What is a Retrieval Augmented Generation system?\",\n",
        "    \"Who authored the QLoRA paper?\",\n",
        "    \"What is the most popular deep learning framework?\",\n",
        "    \"What significant improvements does the LoRA system make?\"\n",
        "]\n",
        "\n",
        "answers = [\n",
        "    {\"must_mention\" : [\"paged\", \"optimizer\"]},\n",
        "    {\"must_mention\" : [\"NF4\", \"NormalFloat\"]},\n",
        "    {\"must_mention\" : [\"ground\", \"context\"]},\n",
        "    {\"must_mention\" : [\"Tim\", \"Dettmers\"]},\n",
        "    {\"must_mention\" : [\"PyTorch\", \"TensorFlow\"]},\n",
        "    {\"must_mention\" : [\"reduce\", \"parameters\"]},\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfMXF2KAsQxs"
      },
      "source": [
        "#### üèóÔ∏è Activity #3:\n",
        "\n",
        "Please create a dataset in the above format with at least 5 questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "CbagRuJop83E"
      },
      "outputs": [],
      "source": [
        "questions = [\n",
        "    \"What optimizer is used in QLoRA?\",\n",
        "    \"What data type was created in the QLoRA paper?\",\n",
        "    \"What is a Retrieval Augmented Generation system?\",\n",
        "    \"Who authored the QLoRA paper?\",\n",
        "    \"What is the most popular deep learning framework?\",\n",
        "    \"What significant improvements does the LoRA system make?\"\n",
        "]\n",
        "\n",
        "answers = [\n",
        "    {\"must_mention\" : [\"paged\", \"optimizer\"]},\n",
        "    {\"must_mention\" : [\"NF4\", \"NormalFloat\"]},\n",
        "    {\"must_mention\" : [\"ground\", \"context\"]},\n",
        "    {\"must_mention\" : [\"Tim\", \"Dettmers\"]},\n",
        "    {\"must_mention\" : [\"PyTorch\", \"TensorFlow\"]},\n",
        "    {\"must_mention\" : [\"reduce\", \"parameters\"]},\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7QVFuAmsh7L"
      },
      "source": [
        "Now we can add our dataset to our LangSmith project using the following code which we saw last Thursday!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "RLfrZrgSsn85"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'example_ids': ['d8adb26e-5a0f-4225-9677-de2d4f341045',\n",
              "  '338265b9-559c-4331-bb7d-9ba2fd7e185b',\n",
              "  'd8817be0-07d8-42aa-baf8-134954ace1b9',\n",
              "  '3b0969ca-3548-46e5-9122-cbb33b88db54',\n",
              "  'a93eb2d8-fbff-4edb-ac17-15a487d18ae1',\n",
              "  'f34c7f71-ab32-4a5f-ae56-30b9e06fa320'],\n",
              " 'count': 6}"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langsmith import Client\n",
        "\n",
        "client = Client()\n",
        "\n",
        "dataset_name = f\"Retrieval Augmented Generation - Evaluation Dataset - {uuid4().hex[0:8]}\"\n",
        "\n",
        "dataset = client.create_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    description=\"Questions about the QLoRA Paper to Evaluate RAG over the same paper.\"\n",
        ")\n",
        "\n",
        "client.create_examples(\n",
        "    inputs=[{\"question\" : q} for q in questions],\n",
        "    outputs=answers,\n",
        "    dataset_id=dataset.id,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lRTXUrTtP9Y"
      },
      "source": [
        "### Task 2: Adding Evaluators\n",
        "\n",
        "Now we can add a custom evaluator to see if our responses contain the expected information.\n",
        "\n",
        "We'll be using a fairly naive exact-match process to determine if our response contains specific strings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "QrAUXMFftlAY"
      },
      "outputs": [],
      "source": [
        "from langsmith.evaluation import EvaluationResult, run_evaluator\n",
        "\n",
        "@run_evaluator\n",
        "def must_mention(run, example) -> EvaluationResult:\n",
        "    prediction = run.outputs.get(\"output\") or \"\"\n",
        "    required = example.outputs.get(\"must_mention\") or []\n",
        "    score = all(phrase in prediction for phrase in required)\n",
        "    return EvaluationResult(key=\"must_mention\", score=score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1RJr349zhv7"
      },
      "source": [
        "Task 3: Evaluating\n",
        "\n",
        "All that is left to do is evaluate our agent's response!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "efcf57067cf743d8b4ce059a61cbe02e",
            "53e33aae3b97490c82aec7bbb0d6ebba",
            "ad84e0e971d3455db2efe7dd0d1f803e",
            "72adef9b70dd48198b7322b6c5b113cf",
            "8a61d045ffd44ac58f3f13eb10044836",
            "041e22a9b5514e36bd4d1dac01d5d398",
            "886d762f2a7c421382efb5502c6d42a1",
            "ab91fd625bbd43afbf8c6398193a88d0",
            "716557ad09874dcb989d75f7c74424cd",
            "77d4c0ebaae045b58efc4f789c9a2360",
            "0d622ccc56264fac8fd7508dbdbe6e29"
          ]
        },
        "id": "p5TeCUUkuGld",
        "outputId": "2f7d62a2-e78d-447a-d07b-f9e4d500fb79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for experiment: 'Search Pipeline - Evaluation - 3b2f-e0acecde' at:\n",
            "https://smith.langchain.com/o/fa493a47-99f1-4723-a121-32a4e602af4a/datasets/017c7892-60af-40c2-af7e-f177a17f80a9/compare?selectedSessions=7ac07b66-0197-4f32-bee6-e3b75a435039\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7016eae470304c168c34a86f0211ad0f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "experiment_results = client.evaluate(\n",
        "    agent_chain_with_formatting,\n",
        "    data=dataset_name,\n",
        "    evaluators=[must_mention],\n",
        "    experiment_prefix=f\"Search Pipeline - Evaluation - {uuid4().hex[0:4]}\",\n",
        "    metadata={\"version\": \"1.0.0\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        },
        "id": "eeEqU7s05Byu",
        "outputId": "78395075-a05d-4ebd-c798-ed968b935318"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<ExperimentResults Search Pipeline - Evaluation - 3b2f-e0acecde>"
            ],
            "text/plain": [
              "<ExperimentResults Search Pipeline - Evaluation - 3b2f-e0acecde>"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "experiment_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhTNe4kWrplB"
      },
      "source": [
        "## Part 2: LangGraph with Helpfulness:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1wKRddbIY_S"
      },
      "source": [
        "### Task 3: Adding Helpfulness Check and \"Loop\" Limits\n",
        "\n",
        "Now that we've done evaluation - let's see if we can add an extra step where we review the content we've generated to confirm if it fully answers the user's query!\n",
        "\n",
        "We're going to make a few key adjustments to account for this:\n",
        "\n",
        "1. We're going to add an artificial limit on how many \"loops\" the agent can go through - this will help us to avoid the potential situation where we never exit the loop.\n",
        "2. We'll add to our existing conditional edge to obtain the behaviour we desire."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npTYJ8ayR5B3"
      },
      "source": [
        "First, let's define our state again - we can check the length of the state object, so we don't need additional state for this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "-LQ84YhyJG0w"
      },
      "outputs": [],
      "source": [
        "class AgentState(TypedDict):\n",
        "  messages: Annotated[list, add_messages]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sD7EV0HqSQcb"
      },
      "source": [
        "Now we can set our graph up! This process will be almost entirely the same - with the inclusion of one additional node/conditional edge!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6rN7feNVn9f"
      },
      "source": [
        "##### YOUR MARKDOWN HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6r6XXA5FJbVf",
        "outputId": "ff713041-e498-4f0f-a875-a03502b87729"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x11bf7bc50>"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "graph_with_helpfulness_check = StateGraph(AgentState)\n",
        "\n",
        "graph_with_helpfulness_check.add_node(\"agent\", call_model)\n",
        "graph_with_helpfulness_check.add_node(\"action\", tool_node)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZ22o2mWVrfp"
      },
      "source": [
        "##### YOUR MARKDOWN HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HNWHwWxuRiLY",
        "outputId": "295f5a35-ceff-452a-ffb8-c52eada6a816"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x11bf7bc50>"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "graph_with_helpfulness_check.set_entry_point(\"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsXeF6xlaXOZ"
      },
      "source": [
        "##### YOUR MARKDOWN HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "z_Sq3A9SaV1O"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "def tool_call_or_helpful(state):\n",
        "  last_message = state[\"messages\"][-1]\n",
        "\n",
        "  if last_message.tool_calls:\n",
        "    return \"action\"\n",
        "\n",
        "  initial_query = state[\"messages\"][0]\n",
        "  final_response = state[\"messages\"][-1]\n",
        "\n",
        "  if len(state[\"messages\"]) > 10:\n",
        "    return \"END\"\n",
        "\n",
        "  prompt_template = \"\"\"\\\n",
        "  Given an initial query and a final response, determine if the final response is extremely helpful or not. Please indicate helpfulness with a 'Y' and unhelpfulness as an 'N'.\n",
        "\n",
        "  Initial Query:\n",
        "  {initial_query}\n",
        "\n",
        "  Final Response:\n",
        "  {final_response}\"\"\"\n",
        "\n",
        "  helpfullness_prompt_template = PromptTemplate.from_template(prompt_template)\n",
        "\n",
        "  helpfulness_check_model = ChatOpenAI(model=\"gpt-4.1-mini\")\n",
        "\n",
        "  helpfulness_chain = helpfullness_prompt_template | helpfulness_check_model | StrOutputParser()\n",
        "\n",
        "  helpfulness_response = helpfulness_chain.invoke({\"initial_query\" : initial_query.content, \"final_response\" : final_response.content})\n",
        "\n",
        "  if \"Y\" in helpfulness_response:\n",
        "    return \"end\"\n",
        "  else:\n",
        "    return \"continue\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fz1u9Vf4SHxJ"
      },
      "source": [
        "#### üèóÔ∏è Activity #4:\n",
        "\n",
        "Please write what is happening in our `tool_call_or_helpful` function!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BhnBW2YVsJO"
      },
      "source": [
        "##### YOUR MARKDOWN HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVTKnWMbP_8T",
        "outputId": "7f729b1f-311c-4084-ceaf-0da437900c85"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x11bf7bc50>"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "graph_with_helpfulness_check.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    tool_call_or_helpful,\n",
        "    {\n",
        "        \"continue\" : \"agent\",\n",
        "        \"action\" : \"action\",\n",
        "        \"end\" : END\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGDLEWOIVtK0"
      },
      "source": [
        "##### YOUR MARKDOWN HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbDK2MbuREgU",
        "outputId": "21a64c20-27a1-4e0e-afde-a639abaa8b55"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x11bf7bc50>"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "graph_with_helpfulness_check.add_edge(\"action\", \"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSI8AOaEVvT-"
      },
      "source": [
        "##### YOUR MARKDOWN HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "oQldl8ERQ8lf"
      },
      "outputs": [],
      "source": [
        "agent_with_helpfulness_check = graph_with_helpfulness_check.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F67FGCMRVwGz"
      },
      "source": [
        "##### YOUR MARKDOWN HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3oo8E-PRK1T",
        "outputId": "f152dea8-96ad-4d29-d8b2-a064c96a8bd3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_YvkCqOBibh2xun8ehhpKUNB2', 'function': {'arguments': '{\"query\": \"LoRA machine learning\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}, {'id': 'call_FIzxsYOb1sPaPPgRgao6lxBJ', 'function': {'arguments': '{\"query\": \"Tim Dettmers\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}, {'id': 'call_CBgMWpXuW3s7tGz9W0bbhfTr', 'function': {'arguments': '{\"query\": \"Attention in machine learning\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 79, 'prompt_tokens': 177, 'total_tokens': 256, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_438cf0ae7a', 'id': 'chatcmpl-CYsjRfxaiUznO0UWxFw3u9Ur217Ke', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--ebb34011-8227-4f64-98de-8d31442807da-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'LoRA machine learning'}, 'id': 'call_YvkCqOBibh2xun8ehhpKUNB2', 'type': 'tool_call'}, {'name': 'tavily_search_results_json', 'args': {'query': 'Tim Dettmers'}, 'id': 'call_FIzxsYOb1sPaPPgRgao6lxBJ', 'type': 'tool_call'}, {'name': 'tavily_search_results_json', 'args': {'query': 'Attention in machine learning'}, 'id': 'call_CBgMWpXuW3s7tGz9W0bbhfTr', 'type': 'tool_call'}], usage_metadata={'input_tokens': 177, 'output_tokens': 79, 'total_tokens': 256, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'action'\n",
            "[ToolMessage(content='[{\"title\": \"What is LoRA (Low-Rank Adaption)? - IBM\", \"url\": \"https://www.ibm.com/think/topics/lora\", \"content\": \"Low-rank adaptation (LoRA) is a technique used to adapt machine learning models to new contexts. LoRA leverages the concept of lower-rank matrices to make the model training process extremely efficient and fast. LoRA adds low-rank matrices to the frozen original machine learning model. Removing this resiliency can make models less accurate, so the rank of update matrices can be tuned as a part of the LoRA process. AI models   Explore IBM Granite  Explore the IBM library of foundation models in the watsonx portfolio to scale generative AI for your business with confidence. Explore the IBM library of foundation models in the IBM watsonx portfolio to scale generative AI for your business with confidence.\", \"score\": 0.98578}, {\"title\": \"LoRA: Low-Rank Adaptation of Large Language Models - arXiv\", \"url\": \"https://arxiv.org/abs/2106.09685\", \"content\": \"> cs > arXiv:2106.09685 **arXiv:2106.09685** (cs) # Title:LoRA: Low-Rank Adaptation of Large Language Models Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen View a PDF of the paper titled LoRA: Low-Rank Adaptation of Large Language Models, by Edward J. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. | Cite as: | arXiv:2106.09685 [cs.CL] | |  | (or  arXiv:2106.09685v2 [cs.CL] for this version) | View a PDF of the paper titled LoRA: Low-Rank Adaptation of Large Language Models, by Edward J. cs   ## BibTeX formatted citation # Bibliographic and Citation Tools\", \"score\": 0.98023}, {\"title\": \"Understanding LoRA with a minimal example - Posit AI Blog\", \"url\": \"https://blogs.rstudio.com/tensorflow/posts/2023-06-22-understanding-lora/\", \"content\": \"LoRA (Low Rank Adaptation) is a new technique for fine-tuning deep learning models that works by reducing the number of trainable parameters and enables efficient task switching. LoRA (Low-Rank Adaptation) is a new technique for fine tuning large scale pre-trained LoRA: Low-Rank Adaptation of Large Language Models are the data and Œò0 the weights from a pre-trained model. 2. Train a full rank linear model to estimate Œ∏ - this will be our ‚Äòpre-trained‚Äô model. 4. Train a low rank model using the pre=trained weights. train <- function(model, X, y, batch_size = 128, epochs = 100) { train(model, X, y) We now initialize the LoRA model. The LoRA model that we are Now let‚Äôs train the lora model on the new distribution:\", \"score\": 0.97072}, {\"title\": \"LoRA - Hugging Face\", \"url\": \"https://huggingface.co/docs/diffusers/main/en/training/lora\", \"content\": \"LoRA (Low-Rank Adaptation of Large Language Models) is a popular and lightweight training technique that significantly reduces the number of trainable parameters. This guide will explore the train\\\\\\\\_text\\\\\\\\_to\\\\\\\\_image\\\\\\\\_lora.py script to help you become more familiar with it, and how you can adapt it for your own use-case. accelerate launch train_text_to_image_lora.py \\\\\\\\ Many of the basic and important parameters are described in the Text-to-image training guide, so this guide just focuses on the LoRA relevant parameters: Aside from setting up the LoRA layers, the training script is more or less the same as train\\\\\\\\_text\\\\\\\\_to\\\\\\\\_image.py! accelerate launch --mixed_precision=\\\\\"fp16\\\\\"  train_text_to_image_lora.py \\\\\\\\ Congratulations on training a new model with LoRA!\", \"score\": 0.96862}, {\"title\": \"LoRA can turn AI models into specialists quickly - IBM Research\", \"url\": \"https://research.ibm.com/blog/LoRAs-explained\", \"content\": \"# Serving customized AI models at scale with LoRA IBM Research is innovating with LoRAs to make AI models easier to customize and serve at scale. ‚ÄúAdding on a LoRA to your lovingly crafted, general-purpose model can make it single-mindedly good at, say, analyzing legal documents, without the computational costs of full fine-tuning,‚Äù says David Cox, VP of AI models at IBM Research. ‚ÄúLoRA has democratized LLM training by giving more people the ability to fine-tune larger models,‚Äù says Alan Ritter, an associate professor at Georgia Tech who has collaborated with the MIT-IBM Watson AI Lab on LoRA-related work. ‚ÄúAlthough one LoRA is only 1% of the model weights, 1,000 LoRAs is 10 times the model‚Äôs size,‚Äù says Mikhail Yurochkin, an IBM researcher who heads a team focused on operationalizing LLMs.\", \"score\": 0.96391}]', name='tavily_search_results_json', id='a283c871-38e5-4004-a1a7-ff8e9eb79d40', tool_call_id='call_YvkCqOBibh2xun8ehhpKUNB2', artifact={'query': 'LoRA machine learning', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://www.ibm.com/think/topics/lora', 'title': 'What is LoRA (Low-Rank Adaption)? - IBM', 'content': 'Low-rank adaptation (LoRA) is a technique used to adapt machine learning models to new contexts. LoRA leverages the concept of lower-rank matrices to make the model training process extremely efficient and fast. LoRA adds low-rank matrices to the frozen original machine learning model. Removing this resiliency can make models less accurate, so the rank of update matrices can be tuned as a part of the LoRA process. AI models   Explore IBM Granite  Explore the IBM library of foundation models in the watsonx portfolio to scale generative AI for your business with confidence. Explore the IBM library of foundation models in the IBM watsonx portfolio to scale generative AI for your business with confidence.', 'score': 0.98578, 'raw_content': None}, {'url': 'https://arxiv.org/abs/2106.09685', 'title': 'LoRA: Low-Rank Adaptation of Large Language Models - arXiv', 'content': '> cs > arXiv:2106.09685 **arXiv:2106.09685** (cs) # Title:LoRA: Low-Rank Adaptation of Large Language Models Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen View a PDF of the paper titled LoRA: Low-Rank Adaptation of Large Language Models, by Edward J. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. | Cite as: | arXiv:2106.09685 [cs.CL] | |  | (or  arXiv:2106.09685v2 [cs.CL] for this version) | View a PDF of the paper titled LoRA: Low-Rank Adaptation of Large Language Models, by Edward J. cs   ## BibTeX formatted citation # Bibliographic and Citation Tools', 'score': 0.98023, 'raw_content': None}, {'url': 'https://blogs.rstudio.com/tensorflow/posts/2023-06-22-understanding-lora/', 'title': 'Understanding LoRA with a minimal example - Posit AI Blog', 'content': 'LoRA (Low Rank Adaptation) is a new technique for fine-tuning deep learning models that works by reducing the number of trainable parameters and enables efficient task switching. LoRA (Low-Rank Adaptation) is a new technique for fine tuning large scale pre-trained LoRA: Low-Rank Adaptation of Large Language Models are the data and Œò0 the weights from a pre-trained model. 2. Train a full rank linear model to estimate Œ∏ - this will be our ‚Äòpre-trained‚Äô model. 4. Train a low rank model using the pre=trained weights. train <- function(model, X, y, batch_size = 128, epochs = 100) { train(model, X, y) We now initialize the LoRA model. The LoRA model that we are Now let‚Äôs train the lora model on the new distribution:', 'score': 0.97072, 'raw_content': None}, {'url': 'https://huggingface.co/docs/diffusers/main/en/training/lora', 'title': 'LoRA - Hugging Face', 'content': 'LoRA (Low-Rank Adaptation of Large Language Models) is a popular and lightweight training technique that significantly reduces the number of trainable parameters. This guide will explore the train\\\\_text\\\\_to\\\\_image\\\\_lora.py script to help you become more familiar with it, and how you can adapt it for your own use-case. accelerate launch train_text_to_image_lora.py \\\\ Many of the basic and important parameters are described in the Text-to-image training guide, so this guide just focuses on the LoRA relevant parameters: Aside from setting up the LoRA layers, the training script is more or less the same as train\\\\_text\\\\_to\\\\_image.py! accelerate launch --mixed_precision=\"fp16\"  train_text_to_image_lora.py \\\\ Congratulations on training a new model with LoRA!', 'score': 0.96862, 'raw_content': None}, {'url': 'https://research.ibm.com/blog/LoRAs-explained', 'title': 'LoRA can turn AI models into specialists quickly - IBM Research', 'content': '# Serving customized AI models at scale with LoRA IBM Research is innovating with LoRAs to make AI models easier to customize and serve at scale. ‚ÄúAdding on a LoRA to your lovingly crafted, general-purpose model can make it single-mindedly good at, say, analyzing legal documents, without the computational costs of full fine-tuning,‚Äù says David Cox, VP of AI models at IBM Research. ‚ÄúLoRA has democratized LLM training by giving more people the ability to fine-tune larger models,‚Äù says Alan Ritter, an associate professor at Georgia Tech who has collaborated with the MIT-IBM Watson AI Lab on LoRA-related work. ‚ÄúAlthough one LoRA is only 1% of the model weights, 1,000 LoRAs is 10 times the model‚Äôs size,‚Äù says Mikhail Yurochkin, an IBM researcher who heads a team focused on operationalizing LLMs.', 'score': 0.96391, 'raw_content': None}], 'response_time': 2.02, 'request_id': 'b050e62c-9dd0-41bd-b7eb-1f6e1873b64a'}), ToolMessage(content='[{\"title\": \"Tim Dettmers | Carnegie Mellon University Computer Science ...\", \"url\": \"https://www.csd.cs.cmu.edu/people/faculty/tim-dettmers\", \"content\": \"# Tim Dettmers **Department**   Machine Learning Department   Computer Science Department **Research Interests**   Tim Dettmers\\' work focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. His main focus is to develop open-source agent systems that are competitive with closed-weight agents and can be run on consumer hardware, such as laptops. His research won oral, spotlight, and best paper awards at conferences such as ICLR and NeurIPS and was awarded the Block Award and Madrona Prize. He created the bitsandbytes open-source library for efficient foundation models, which is growing at 2.2 million installations per month, and for which he received Google Open Source and PyTorch Foundation awards. ## Research Statement My current research interests include:   - open-source agents  \", \"score\": 0.98593}, {\"title\": \"About Me - Tim Dettmers\", \"url\": \"https://timdettmers.com/about/\", \"content\": \"I have a PhD from University of Washington advised by Luke Zettlemoyer working on efficient deep learning at the intersection between machine learning, natural language processing, and computer systems with a focus on quantization and sparsity. I do this by making large models accessible through my research (QLoRA, LLM.int8(), k-bit inference scaling laws, Petals, SWARM) and by developing software that makes it easy to use my research innovations (bitsandbytes). My main research thesis is that computational efficient methods will accelerate and enable progress in and understanding of deep learning. **Tim Dettmers**, Luke Zettlemoyer. **Tim Dettmers**,\\xa0Mike Lewis,\\xa0Younes Belkada,\\xa0Luke Zettlemoyer. **Tim Dettmers**, Luke Zettlemoyer. 8-Bit Approximations for Parallelism in Deep Learning,\\xa0**Tim** Dettmers. 2021\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0NeurIPS 2021 Best Reviewer Award\", \"score\": 0.98593}, {\"title\": \"Author: Tim Dettmers | NVIDIA Technical Blog\", \"url\": \"https://developer.nvidia.com/blog/author/tdettmers/\", \"content\": \"This series of blog posts aims to provide an intuitive and gentle introduction to deep learning that does not rely heavily on math or theoretical... We and our third-party partners also use cookies and other tools to collect and record information you provide as well as information about your interactions with our websites for performance improvement, analytics, and to assist in marketing efforts. To opt out of non-cookie personal information \\\\\"sales\\\\\" / \\\\\"sharing\\\\\" for targeted advertising purposes, please visit the NVIDIA Preference Center. We have also opted you out of \\\\\"sharing\\\\\"/\\\\\"sales\\\\\" of personal information outside of cookies which overrides at least one of your previous settings.\", \"score\": 0.98269}, {\"title\": \"Tim Dettmers ‚Äî Making deep learning accessible.\", \"url\": \"https://timdettmers.com/\", \"content\": \"[[Read more‚Ä¶] about Which GPU(s) to Get for Deep Learning: My Experience and Advice for Using GPUs in Deep Learning](https://timdettmers.com/2023/01/30/which-gpu-for-deep-learning/#more-6) In this blog post, I will discuss this career-centered perspective in detail, but I also provide you with three other views that hopefully help you make a balanced choice that not only leads to academic success but long-term satisfaction and a full and rich life. [[Read more‚Ä¶] about A Full Hardware Guide to Deep Learning](https://timdettmers.com/2018/12/16/deep-learning-hardware-guide/#more-121) [[Read more‚Ä¶] about Machine Learning PhD Applications ‚Äî Everything You Need to Know](https://timdettmers.com/2018/11/26/phd-applications/#more-710) [[Read more‚Ä¶] about Deep Learning Hardware Limbo](https://timdettmers.com/2017/12/21/deep-learning-hardware-limbo/#more-627) This morning I got an email about my blog post discussing the history of deep learning which rattled me back into a time of my academic career which I rather not think about. [[Read more‚Ä¶] about Credit Assignment in Deep Learning](https://timdettmers.com/2017/09/16/credit-assignment-deep-learning/#more-596)\", \"score\": 0.98242}, {\"title\": \"Tim Dettmers - AI2050 - Schmidt Sciences\", \"url\": \"https://ai2050.schmidtsciences.org/fellow/tim-dettmers/\", \"content\": \"Tim Dettmers is an Assistant Professor at Carnegie Mellon University and a Research Scientist at the Allen Institute for AI, and his research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. AI models like ChatGPT work well for general use but fail in specialized expert domains, such as the medical sciences. To make AI models work in expert domains, one must adapt them, which is costly and requires significant AI expertise. This project overcomes these cost and expertise barriers through two new approaches: (1) use AI models themselves to perform the AI model adaptation process automatically; (2) make the adaptation process cheap so it can be run on regular consumer hardware.\", \"score\": 0.9754}]', name='tavily_search_results_json', id='4c4c4871-06b7-4c4d-b574-0e7a1cbbe824', tool_call_id='call_FIzxsYOb1sPaPPgRgao6lxBJ', artifact={'query': 'Tim Dettmers', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://www.csd.cs.cmu.edu/people/faculty/tim-dettmers', 'title': 'Tim Dettmers | Carnegie Mellon University Computer Science ...', 'content': \"# Tim Dettmers **Department**   Machine Learning Department   Computer Science Department **Research Interests**   Tim Dettmers' work focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. His main focus is to develop open-source agent systems that are competitive with closed-weight agents and can be run on consumer hardware, such as laptops. His research won oral, spotlight, and best paper awards at conferences such as ICLR and NeurIPS and was awarded the Block Award and Madrona Prize. He created the bitsandbytes open-source library for efficient foundation models, which is growing at 2.2 million installations per month, and for which he received Google Open Source and PyTorch Foundation awards. ## Research Statement My current research interests include:   - open-source agents  \", 'score': 0.98593, 'raw_content': None}, {'url': 'https://timdettmers.com/about/', 'title': 'About Me - Tim Dettmers', 'content': 'I have a PhD from University of Washington advised by Luke Zettlemoyer working on efficient deep learning at the intersection between machine learning, natural language processing, and computer systems with a focus on quantization and sparsity. I do this by making large models accessible through my research (QLoRA, LLM.int8(), k-bit inference scaling laws, Petals, SWARM) and by developing software that makes it easy to use my research innovations (bitsandbytes). My main research thesis is that computational efficient methods will accelerate and enable progress in and understanding of deep learning. **Tim Dettmers**, Luke Zettlemoyer. **Tim Dettmers**,\\xa0Mike Lewis,\\xa0Younes Belkada,\\xa0Luke Zettlemoyer. **Tim Dettmers**, Luke Zettlemoyer. 8-Bit Approximations for Parallelism in Deep Learning,\\xa0**Tim** Dettmers. 2021\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0NeurIPS 2021 Best Reviewer Award', 'score': 0.98593, 'raw_content': None}, {'url': 'https://developer.nvidia.com/blog/author/tdettmers/', 'title': 'Author: Tim Dettmers | NVIDIA Technical Blog', 'content': 'This series of blog posts aims to provide an intuitive and gentle introduction to deep learning that does not rely heavily on math or theoretical... We and our third-party partners also use cookies and other tools to collect and record information you provide as well as information about your interactions with our websites for performance improvement, analytics, and to assist in marketing efforts. To opt out of non-cookie personal information \"sales\" / \"sharing\" for targeted advertising purposes, please visit the NVIDIA Preference Center. We have also opted you out of \"sharing\"/\"sales\" of personal information outside of cookies which overrides at least one of your previous settings.', 'score': 0.98269, 'raw_content': None}, {'url': 'https://timdettmers.com/', 'title': 'Tim Dettmers ‚Äî Making deep learning accessible.', 'content': '[[Read more‚Ä¶] about Which GPU(s) to Get for Deep Learning: My Experience and Advice for Using GPUs in Deep Learning](https://timdettmers.com/2023/01/30/which-gpu-for-deep-learning/#more-6) In this blog post, I will discuss this career-centered perspective in detail, but I also provide you with three other views that hopefully help you make a balanced choice that not only leads to academic success but long-term satisfaction and a full and rich life. [[Read more‚Ä¶] about A Full Hardware Guide to Deep Learning](https://timdettmers.com/2018/12/16/deep-learning-hardware-guide/#more-121) [[Read more‚Ä¶] about Machine Learning PhD Applications ‚Äî Everything You Need to Know](https://timdettmers.com/2018/11/26/phd-applications/#more-710) [[Read more‚Ä¶] about Deep Learning Hardware Limbo](https://timdettmers.com/2017/12/21/deep-learning-hardware-limbo/#more-627) This morning I got an email about my blog post discussing the history of deep learning which rattled me back into a time of my academic career which I rather not think about. [[Read more‚Ä¶] about Credit Assignment in Deep Learning](https://timdettmers.com/2017/09/16/credit-assignment-deep-learning/#more-596)', 'score': 0.98242, 'raw_content': None}, {'url': 'https://ai2050.schmidtsciences.org/fellow/tim-dettmers/', 'title': 'Tim Dettmers - AI2050 - Schmidt Sciences', 'content': 'Tim Dettmers is an Assistant Professor at Carnegie Mellon University and a Research Scientist at the Allen Institute for AI, and his research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. AI models like ChatGPT work well for general use but fail in specialized expert domains, such as the medical sciences. To make AI models work in expert domains, one must adapt them, which is costly and requires significant AI expertise. This project overcomes these cost and expertise barriers through two new approaches: (1) use AI models themselves to perform the AI model adaptation process automatically; (2) make the adaptation process cheap so it can be run on regular consumer hardware.', 'score': 0.9754, 'raw_content': None}], 'response_time': 3.77, 'request_id': '363ad36a-4f96-44f1-b65a-8075740354e8'}), ToolMessage(content='[{\"title\": \"Attention Mechanisms and Their Applications to Complex Systems\", \"url\": \"https://pmc.ncbi.nlm.nih.gov/articles/PMC7996841/\", \"content\": \"Generally formulated, attention in machine learning is a sequential process in which a learning task is guided by a set of elements of the input source (or memory). This is achieved by integrating the attention value into the task. [...] Attention mechanisms have provided and will provide a paradigm shift in machine learning. Specifically, this change is from traditional large-scale vector transformations to more conscious processes (i.e., that focus only on a set of elements), e.g., decomposing a problem into a sequence of attention based reasoning tasks [13,30,31,32,33,34]. [...] Attention mechanisms have provided and will provide a paradigm shift in machine learning [11,12]. These mechanisms allow a model to focus only on a set of elements and to decompose a problem into a sequence of attention based reasoning tasks . Moreover, they can be applied to model complex systems in a flexible and promising way. When it comes to their application, information processing in the system and internal structure are crucial.\", \"score\": 0.9581988}, {\"title\": \"Attention (machine learning) - Wikipedia\", \"url\": \"https://en.wikipedia.org/wiki/Attention_(machine_learning)\", \"content\": \"In machine learning, attention is a method that determines the importance of each component in a sequence relative to the other components in that sequence. In natural language processing, importance is represented by \\\\\"soft\\\\\" weights assigned to each word in a sentence. More generally, attention encodes vectors \\\\\"Vector (mathematics and physics)\\\\\") called token embeddings across a fixed-width sequence that can range from tens to millions of tokens in size. [...] Attention is widely used in natural language processing, computer vision, and speech recognition. In NLP, it improves context understanding in tasks like question answering and summarization. In vision, visual attention helps models focus on relevant image regions, enhancing object detection and image captioning.\\\\n\\\\n### Attention maps as explanations for vision transformers\\\\n\\\\n[edit&action=edit&section=9 \\\\\"Edit section: Attention maps as explanations for vision transformers\\\\\")] [...] Inspired by ideas about attention in humans, the attention mechanism was developed to address the weaknesses of using information from the hidden layers of recurrent neural networks. Recurrent neural networks favor more recent information contained in words at the end of a sentence, while information earlier in the sentence tends to be attenuated. Attention allows a token equal access to any part of a sentence directly, rather than only through the previous state.\\\\n\\\\n## History\", \"score\": 0.9549888}, {\"title\": \"What is an attention mechanism? - IBM\", \"url\": \"https://www.ibm.com/think/topics/attention-mechanism\", \"content\": \"An attention mechanism is a machine learning technique that directs deep learning models to prioritize (or attend to) the most relevant parts of input data. Innovation in attention mechanisms enabled the transformer architecture that yielded the modern large language models (LLMs) that power popular applications like ChatGPT.\", \"score\": 0.88178825}, {\"title\": \"What is Attention Mechanism? - H2O.ai\", \"url\": \"https://h2o.ai/wiki/attention-mechanism/\", \"content\": \"Overall, attention mechanisms play a crucial role in machine learning and artificial intelligence by enabling models to focus on relevant information and improve their performance. Its applications span various domains and tasks, making it a valuable tool for data scientists and practitioners. H2O.ai users can benefit from incorporating attention mechanisms into their workflows to enhance the capabilities and outcomes of their machine-learning models. [...] Artifacts\\\\n Transfer Learning\\\\n\\\\n# Attention Mechanism\\\\n\\\\n## What is an Attention Mechanism?\\\\n\\\\nAn attention mechanism is a technique used in machine learning and artificial intelligence to improve the performance of models by focusing on relevant information. It allows models to selectively attend to different parts of the input data, assigning varying degrees of importance or weight to different elements.\\\\n\\\\n## How Attention Mechanisms Work [...] Attention mechanism is important in machine learning and artificial intelligence for several reasons:\", \"score\": 0.8467682}, {\"title\": \"Attention in the Human Brain and Its Applications in ML - The Gradient\", \"url\": \"https://thegradient.pub/attention-in-human-brain-and-its-applications-in-ml/\", \"content\": \"The importance weighing process of attention is intuitive from a machine learning perspective. Not all parts of the input (or encoded input, extracted features, embedding, etc.) have the same importance in generating (decoding) expected output. [...] We can construct saliency from bottom-up, i.e. a region that stimulates us through color, direction, and intensity and we put our fovea onto that region, or top-down, i.e. we select to attend a subset of input based on its importance. The importance is defined by the context of the task. For more details please see Koch and Ullman‚Äôs seminal work [\\\\\\\\[8\\\\\\\\]]( or [\\\\\\\\[9\\\\\\\\]](\\\\n\\\\nAttention in Machine Learning [...] I assume the start of attention mechanisms in machine learning with the paper \\\\\"Neural Machine Translation by Jointly Learning to Align and Translate\\\\\" by Bahdanau, Cho and Bengio [\\\\\\\\[10:1\\\\\\\\]]( I will explain another attention mechanism approach in computer vision, but the mainstream attention mechanism (which is very popular nowadays) first appeared in this paper.\\\\n\\\\nImage 4\\\\n\\\\nBahdanau et al. proposed a context vector before decoding.\", \"score\": 0.78866494}]', name='tavily_search_results_json', id='b5dfe3ec-f6d9-4984-94de-33fe06196751', tool_call_id='call_CBgMWpXuW3s7tGz9W0bbhfTr', artifact={'query': 'Attention in machine learning', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://pmc.ncbi.nlm.nih.gov/articles/PMC7996841/', 'title': 'Attention Mechanisms and Their Applications to Complex Systems', 'content': 'Generally formulated, attention in machine learning is a sequential process in which a learning task is guided by a set of elements of the input source (or memory). This is achieved by integrating the attention value into the task. [...] Attention mechanisms have provided and will provide a paradigm shift in machine learning. Specifically, this change is from traditional large-scale vector transformations to more conscious processes (i.e., that focus only on a set of elements), e.g., decomposing a problem into a sequence of attention based reasoning tasks [13,30,31,32,33,34]. [...] Attention mechanisms have provided and will provide a paradigm shift in machine learning [11,12]. These mechanisms allow a model to focus only on a set of elements and to decompose a problem into a sequence of attention based reasoning tasks . Moreover, they can be applied to model complex systems in a flexible and promising way. When it comes to their application, information processing in the system and internal structure are crucial.', 'score': 0.9581988, 'raw_content': None}, {'url': 'https://en.wikipedia.org/wiki/Attention_(machine_learning)', 'title': 'Attention (machine learning) - Wikipedia', 'content': 'In machine learning, attention is a method that determines the importance of each component in a sequence relative to the other components in that sequence. In natural language processing, importance is represented by \"soft\" weights assigned to each word in a sentence. More generally, attention encodes vectors \"Vector (mathematics and physics)\") called token embeddings across a fixed-width sequence that can range from tens to millions of tokens in size. [...] Attention is widely used in natural language processing, computer vision, and speech recognition. In NLP, it improves context understanding in tasks like question answering and summarization. In vision, visual attention helps models focus on relevant image regions, enhancing object detection and image captioning.\\n\\n### Attention maps as explanations for vision transformers\\n\\n[edit&action=edit&section=9 \"Edit section: Attention maps as explanations for vision transformers\")] [...] Inspired by ideas about attention in humans, the attention mechanism was developed to address the weaknesses of using information from the hidden layers of recurrent neural networks. Recurrent neural networks favor more recent information contained in words at the end of a sentence, while information earlier in the sentence tends to be attenuated. Attention allows a token equal access to any part of a sentence directly, rather than only through the previous state.\\n\\n## History', 'score': 0.9549888, 'raw_content': None}, {'url': 'https://www.ibm.com/think/topics/attention-mechanism', 'title': 'What is an attention mechanism? - IBM', 'content': 'An attention mechanism is a machine learning technique that directs deep learning models to prioritize (or attend to) the most relevant parts of input data. Innovation in attention mechanisms enabled the transformer architecture that yielded the modern large language models (LLMs) that power popular applications like ChatGPT.', 'score': 0.88178825, 'raw_content': None}, {'url': 'https://h2o.ai/wiki/attention-mechanism/', 'title': 'What is Attention Mechanism? - H2O.ai', 'content': 'Overall, attention mechanisms play a crucial role in machine learning and artificial intelligence by enabling models to focus on relevant information and improve their performance. Its applications span various domains and tasks, making it a valuable tool for data scientists and practitioners. H2O.ai users can benefit from incorporating attention mechanisms into their workflows to enhance the capabilities and outcomes of their machine-learning models. [...] Artifacts\\n Transfer Learning\\n\\n# Attention Mechanism\\n\\n## What is an Attention Mechanism?\\n\\nAn attention mechanism is a technique used in machine learning and artificial intelligence to improve the performance of models by focusing on relevant information. It allows models to selectively attend to different parts of the input data, assigning varying degrees of importance or weight to different elements.\\n\\n## How Attention Mechanisms Work [...] Attention mechanism is important in machine learning and artificial intelligence for several reasons:', 'score': 0.8467682, 'raw_content': None}, {'url': 'https://thegradient.pub/attention-in-human-brain-and-its-applications-in-ml/', 'title': 'Attention in the Human Brain and Its Applications in ML - The Gradient', 'content': 'The importance weighing process of attention is intuitive from a machine learning perspective. Not all parts of the input (or encoded input, extracted features, embedding, etc.) have the same importance in generating (decoding) expected output. [...] We can construct saliency from bottom-up, i.e. a region that stimulates us through color, direction, and intensity and we put our fovea onto that region, or top-down, i.e. we select to attend a subset of input based on its importance. The importance is defined by the context of the task. For more details please see Koch and Ullman‚Äôs seminal work [\\\\[8\\\\]]( or [\\\\[9\\\\]](\\n\\nAttention in Machine Learning [...] I assume the start of attention mechanisms in machine learning with the paper \"Neural Machine Translation by Jointly Learning to Align and Translate\" by Bahdanau, Cho and Bengio [\\\\[10:1\\\\]]( I will explain another attention mechanism approach in computer vision, but the mainstream attention mechanism (which is very popular nowadays) first appeared in this paper.\\n\\nImage 4\\n\\nBahdanau et al. proposed a context vector before decoding.', 'score': 0.78866494, 'raw_content': None}], 'response_time': 1.86, 'request_id': '7f247ab6-1042-4c6c-b34f-4eb4a5957bdc'})]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content=\"Here's a summary of the information I found:\\n\\n1. **LoRA (Low-Rank Adaptation)**: LoRA is a technique used to adapt large machine learning models efficiently. It leverages lower-rank matrices to make the training process faster and more resource-efficient. LoRA adds low-rank matrices to a pre-trained model, allowing for effective fine-tuning without the need for extensive retraining. It performs on par or better than traditional fine-tuning methods while requiring fewer trainable parameters. LoRA is particularly useful for customizing AI models for specific tasks quickly and cost-effectively. [More info](https://www.ibm.com/think/topics/lora), [arXiv paper](https://arxiv.org/abs/2106.09685), [Posit AI Blog](https://blogs.rstudio.com/tensorflow/posts/2023-06-22-understanding-lora/), [Hugging Face](https://huggingface.co/docs/diffusers/main/en/training/lora), [IBM Research](https://research.ibm.com/blog/LoRAs-explained)\\n\\n2. **Tim Dettmers**: Tim Dettmers is a researcher and professor focused on making foundation models like ChatGPT more accessible by reducing their resource requirements. He has developed open-source tools such as the bitsandbytes library for efficient foundation models and works on making AI models more affordable and easier to use on consumer hardware. His work also includes developing methods for efficient deep learning, quantization, and sparsity. [More about Tim Dettmers](https://www.csd.cs.cmu.edu/people/faculty/tim-dettmers), [about him](https://timdettmers.com/about/), [AI2050 profile](https://ai2050.schmidtsciences.org/fellow/tim-dettmers/)\\n\\n3. **Attention in Machine Learning**: Attention mechanisms are techniques that allow models to focus on the most relevant parts of the input data. They assign importance weights to different elements, enabling better context understanding and performance in tasks like natural language processing, computer vision, and speech recognition. Attention has been a paradigm shift, moving from traditional vector transformations to more focused, element-wise processing. It is fundamental to the architecture of modern large language models like transformers. [More about attention](https://en.wikipedia.org/wiki/Attention_(machine_learning)), [IBM explanation](https://www.ibm.com/think/topics/attention-mechanism), [The Gradient article](https://thegradient.pub/attention-in-human-brain-and-its-applications-in-ml/)\\n\\nWould you like more detailed information on any of these topics?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 540, 'prompt_tokens': 3851, 'total_tokens': 4391, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_438cf0ae7a', 'id': 'chatcmpl-CYsjX9XKwRLF4UOntHwlDSSo4zhwo', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--bc833b3e-de30-4d5c-9754-8653b94e8230-0', usage_metadata={'input_tokens': 3851, 'output_tokens': 540, 'total_tokens': 4391, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "inputs = {\"messages\" : [HumanMessage(content=\"Related to machine learning, what is LoRA? Also, who is Tim Dettmers? Also, what is Attention?\")]}\n",
        "\n",
        "async for chunk in agent_with_helpfulness_check.astream(inputs, stream_mode=\"updates\"):\n",
        "    for node, values in chunk.items():\n",
        "        print(f\"Receiving update from node: '{node}'\")\n",
        "        print(values[\"messages\"])\n",
        "        print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVmZPs6lnpsM"
      },
      "source": [
        "### Task 4: LangGraph for the \"Patterns\" of GenAI\n",
        "\n",
        "Let's ask our system about the 4 patterns of Generative AI:\n",
        "\n",
        "1. Prompt Engineering\n",
        "2. RAG\n",
        "3. Fine-tuning\n",
        "4. Agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "ZoLl7GlXoae-"
      },
      "outputs": [],
      "source": [
        "patterns = [\"prompt engineering\", \"RAG\", \"fine-tuning\", \"LLM-based agents\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zkh0YJuCp3Zl",
        "outputId": "d847426e-71b3-47e6-b1ae-351a78d68d1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt engineering is the process of designing and refining prompts to effectively communicate with AI language models, such as GPT, to obtain desired responses. It involves crafting prompts that guide the model to generate accurate, relevant, and useful outputs. This practice has become increasingly important as AI models are integrated into various applications, requiring users to understand how to interact with them optimally.\n",
            "\n",
            "Prompt engineering gained significant prominence around 2020-2021, coinciding with the rise of large language models like GPT-3. As these models became more powerful and widely accessible, the need for effective prompt design to harness their capabilities also grew. The term and practice have continued to evolve, becoming a specialized skill within AI and machine learning communities.\n",
            "\n",
            "Would you like more detailed information on the history or techniques of prompt engineering?\n",
            "\n",
            "\n",
            "\n",
            "RAG, which stands for Retrieval-Augmented Generation, is a technique in natural language processing that combines pre-trained language models with information retrieval systems. This approach allows models to access and incorporate external knowledge from a large corpus of documents or databases during the generation process, leading to more accurate and contextually relevant responses.\n",
            "\n",
            "RAG was introduced by Facebook AI Research (FAIR) in a paper published in 2020. It marked a significant advancement in the field by enabling language models to effectively retrieve and utilize external information, thereby improving their performance on tasks that require factual accuracy and up-to-date knowledge.\n",
            "\n",
            "Would you like a more detailed explanation of how RAG works or its impact on the field?\n",
            "\n",
            "\n",
            "\n",
            "Fine-tuning is a process in machine learning where a pre-trained model is further trained on a specific dataset to adapt it to a particular task or domain. This approach leverages the knowledge the model has already acquired during its initial training on large, general datasets, and then refines it to improve performance on specialized tasks. Fine-tuning typically involves adjusting the model's weights slightly, often with a smaller learning rate, to optimize its performance for the new task without overfitting.\n",
            "\n",
            "Fine-tuning has been a common practice in machine learning for many years, especially in natural language processing and computer vision. However, it gained significant prominence and broke onto the mainstream scene with the advent of large pre-trained models like BERT (Bidirectional Encoder Representations from Transformers) in 2018, and later with models like GPT-2 in 2019 and GPT-3 in 2020. These models demonstrated that starting with a large, pre-trained model and fine-tuning it for specific applications could achieve state-of-the-art results across various tasks, revolutionizing the field.\n",
            "\n",
            "Would you like me to find more detailed historical information or recent developments related to fine-tuning?\n",
            "\n",
            "\n",
            "\n",
            "LLM-based agents are intelligent systems that leverage large language models (LLMs) to perform a variety of tasks, such as understanding natural language, generating human-like responses, and making decisions or taking actions based on the input they receive. These agents can be used in applications like chatbots, virtual assistants, automated customer support, and more complex decision-making systems.\n",
            "\n",
            "The concept of LLM-based agents gained significant attention and broke onto the scene around 2020-2021, coinciding with the development and release of large-scale language models like OpenAI's GPT-3 in 2020. GPT-3's impressive capabilities demonstrated the potential of large language models to serve as the core component of autonomous agents capable of performing complex language understanding and generation tasks.\n",
            "\n",
            "Since then, the field has rapidly evolved, with many new models, techniques, and applications emerging, making LLM-based agents a prominent area of research and development in artificial intelligence.\n",
            "\n",
            "Would you like more detailed information on the history, technical aspects, or recent advancements of LLM-based agents?\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for pattern in patterns:\n",
        "  what_is_string = f\"What is {pattern} and when did it break onto the scene??\"\n",
        "  inputs = {\"messages\" : [HumanMessage(content=what_is_string)]}\n",
        "  messages = agent_with_helpfulness_check.invoke(inputs)\n",
        "  print(messages[\"messages\"][-1].content)\n",
        "  print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
