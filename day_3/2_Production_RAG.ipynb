{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fa_QpI0RXQKx"
      },
      "source": [
        "# LangSmith and Evaluation Overview with AI Makerspace\n",
        "\n",
        "Today we'll be looking at an amazing tool:\n",
        "\n",
        "[LangSmith](https://docs.smith.langchain.com/)!\n",
        "\n",
        "This tool will help us monitor, test, debug, and evaluate our LangChain applications - and more!\n",
        "\n",
        "We'll also be looking at a few Advanced Retrieval techniques along the way - and evaluate it using LangSmith!\n",
        "\n",
        "✋BREAKOUT ROOM #2:\n",
        "- Task 1: Dependencies and OpenAI API Key\n",
        "- Task 2: LangGraph RAG\n",
        "- Task 3: Setting Up LangSmith\n",
        "- Task 4: Examining the Trace in LangSmith!\n",
        "- Task 5: Create Testing Dataset\n",
        "- Task 6: Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tw5ok9p-XuUs"
      },
      "source": [
        "## Task 1: Dependencies and OpenAI API Key\n",
        "\n",
        "We'll be using OpenAI's suite of models today to help us generate and embed our documents for our simple RAG system that leverages Loand Complaint data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADl8-whIAUHD",
        "outputId": "927fd78b-8510-4bea-9e68-a3c74d3eb5a1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "923xinz42sWV"
      },
      "source": [
        "#### Asyncio Bug Handling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "m9U0SbQN2sWc"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cx2AOb-QHwJm"
      },
      "source": [
        "## Task #2: Create a Simple RAG Application Using LangGraph\n",
        "\n",
        "Let's remake our LangGraph RAG pipeline from the first notebook!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYJB3IxEwpnh"
      },
      "source": [
        "## LangGraph Powered RAG\n",
        "\n",
        "First and foremost, LangChain provides a convenient way to store our chunks and their embeddings.\n",
        "\n",
        "It's called a `VectorStore`!\n",
        "\n",
        "We'll be using QDrant as our `VectorStore` today. You can read more about it [here](https://qdrant.tech/documentation/).\n",
        "\n",
        "Think of a `VectorStore` as a smart way to house your chunks and their associated embedding vectors. The implementation of the `VectorStore` also allows for smarter and more efficient search of our embedding vectors - as the method we used above would not scale well as we got into the millions of chunks.\n",
        "\n",
        "Otherwise, the process remains relatively similar under the hood!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBErqPRgxgZR"
      },
      "source": [
        "### Data Collection\n",
        "\n",
        "We'll be leveraging the `DirectoryLoader` to load our PDFs!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-AHA9L3Jxo3r",
        "outputId": "c3535941-ce13-4ddd-ef08-b6b4ab1e507b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'case of any especially tenacious “demon,” he could without much\\ndifficulty be merged into a Christian saint or devil. There was no\\norganized priesthood to be overcome, the primitive religious\\nobservances consisting almost entirely of occasional orgies presided\\nover by an old woman, who filled the priestly offices of interpreter for\\nthe unseen powers and chief eater at the sacrificial feast. With their\\nunflagging zeal, their organization, their elaborate forms and\\nceremonies, the missionaries were enabled to win the confidence of\\nthe natives, especially as the greater part of them learned the local\\nlanguage and identified their lives with the communities under their\\ncare. Accordingly, the people took kindly to their new teachers and\\nrulers, so that in less than a generation Spanish authority was\\ngenerally recognized in the settled portions of the Philippines, and in\\nthe succeeding years the missionaries gradually extended this area\\nby forming settlements from among the wilder peoples, whom they\\npersuaded to abandon the more objectionable features of their old\\nroving, often predatory, life and to group themselves into towns and\\nvillages “under the bell.”\\nThe tactics employed in the conquest and the subsequent\\nbehavior of the conquerors were true to the old Spanish nature, so\\nsuccinctly characterized by a plain-spoken Englishman of Mary’s\\nreign, when the war-cry of Castile encircled the globe and even\\nhovered ominously near the “sceptered isle,” when in the intoxication\\nof power character stands out so sharply defined: “They be verye\\nwyse and politicke, and can, thorowe ther wysdome, reform and\\nbrydell theyr owne natures for a tyme, and applye ther conditions to\\nthe manners of those men with whom they meddell gladlye by\\nfriendshippe; whose mischievous maners a man shall never know\\nuntyll he come under ther subjection; but then shall he parfectlye\\nparceve and fele them: for in dissimulations untyll they have ther\\npurposes, and afterwards in oppression and tyrannye, when they\\ncan obtain them, they do exceed all other nations upon the earthe.”ൢ\\nIn the working out of this spirit, with all the indomitable courage\\nand fanatical ardor derived from the long contests with the Moors,\\nthey reduced the native peoples to submission, but still not to the\\ngalling yoke which they fastened upon the aborigines of America, to\\nmake one Las Casas shine amid the horde of Pizarros. There was'"
            ]
          },
          "execution_count": 3,
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_community.document_loaders import DirectoryLoader\n",
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "\n",
        "directory_loader = DirectoryLoader(\"data\", glob=\"**/*.pdf\", loader_cls=PyMuPDFLoader)\n",
        "\n",
        "jr_document = directory_loader.load()\n",
        "\n",
        "jr_document[5].page_content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uH7ZPVJLx6Cn"
      },
      "source": [
        "### Chunking Our Documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsSCRQUSyBKT"
      },
      "source": [
        "Let's do the same process as we did before with our `RecursiveCharacterTextSplitter` - but this time we'll use ~200 tokens as our max chunk size!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "execution_count": 4,
      "metadata": {
        "id": "SzolG1FLx2f_"
      },
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "def tiktoken_len(text):\n",
        "    tokens = tiktoken.encoding_for_model(\"gpt-4o\").encode(\n",
        "        text,\n",
        "    )\n",
        "    return len(tokens)\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 750,\n",
        "    chunk_overlap = 0,\n",
        "    length_function = tiktoken_len,\n",
        ")\n",
        "jose_rizal_chunks = text_splitter.split_documents(jr_document)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpV4f1eXyXVJ",
        "outputId": "a666d551-6b20-4761-82ad-6f9b524d1660"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "812"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(jose_rizal_chunks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTJ60Ck6ybe_"
      },
      "source": [
        "Let's verify the process worked as intended by checking our max document length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "950mB338yZR8",
        "outputId": "9813bfb9-cf80-4787-c962-69a5b1ccdff6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "722\n"
          ]
        }
      ],
      "source": [
        "max_chunk_length = 0\n",
        "\n",
        "for chunk in jose_rizal_chunks:\n",
        "  max_chunk_length = max(max_chunk_length, tiktoken_len(chunk.page_content))\n",
        "\n",
        "print(max_chunk_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDt3RufQy1cP"
      },
      "source": [
        "Perfect! Now we can carry on to creating and storing our embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kocCe4zLy5qT"
      },
      "source": [
        "### Embeddings and Vector Storage\n",
        "\n",
        "We'll use the `text-embedding-3-small` embedding model again - and `Qdrant` to store all our embedding vectors for easy retrieval later!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "7M0X1eVlWPFf"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import Qdrant\n",
        "from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "\n",
        "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "qdrant_vectorstore = Qdrant.from_documents(\n",
        "    documents=jose_rizal_chunks,\n",
        "    embedding=embedding_model,\n",
        "    location=\":memory:\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-NDvjfzXhVp"
      },
      "source": [
        "Now let's set up our retriever, just as we saw before, but this time using LangChain's simple `as_retriever()` method!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Edjx19YBXavZ"
      },
      "outputs": [],
      "source": [
        "qdrant_retriever = qdrant_vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1OM0DiYcOj-"
      },
      "source": [
        "#### Back to the Flow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7apXaEBzQai"
      },
      "source": [
        "We're ready to move to the next step!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMhcU37dzV5k"
      },
      "source": [
        "### Setting up our RAG\n",
        "\n",
        "We'll use the same LangGraph pipeline we created in the first notebook. \n",
        "\n",
        "Let's think through each part:\n",
        "\n",
        "1. First we need to retrieve context\n",
        "2. We need to pipe that context to our model\n",
        "3. We need to parse that output\n",
        "\n",
        "Let's start by setting up our prompt again, just so it's fresh in our minds!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oatgDa7cOXWV"
      },
      "source": [
        "Complete the prompt so that your RAG application answers queries based on the context provided, but *does not* answer queries if the context is unrelated to the query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "TE5tick_YPJj"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "HUMAN_TEMPLATE = \"\"\"\n",
        "You are a knowledgeable assistant that answers questions based strictly on provided context.\n",
        "Instructions:\n",
        "- Only answer questions using information from the provided context\n",
        "- If the context doesn't contain relevant information, respond with \"I don't know\"\n",
        "- Be accurate and cite specific parts of the context when possible\n",
        "- Keep responses comprehensive and detailed.\n",
        "- Only use the provided context. Do not use external knowledge.\n",
        "- Only provide answers when you are confident the context supports your response.\n",
        "\n",
        "#CONTEXT:\n",
        "{context}\n",
        "\n",
        "QUERY:\n",
        "{query}\n",
        "\n",
        "Please provide your answer based solely on the context above.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "#Use the provide context to answer the provided user query. Only use the provided context to answer the query. If you do not know the answer, or it's not contained in the provided context respond with \"I don't know\"\n",
        "\n",
        "# \n",
        "chat_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"human\", HUMAN_TEMPLATE)\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DI2tNXIT1iuB"
      },
      "source": [
        "We'll set our Generator - `gpt-4.1-nano` in this case - below!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "rZ-9gF1x1iEz"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "openai_chat_model = ChatOpenAI(model=\"gpt-4o-mini\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZKadufhc2RL"
      },
      "source": [
        "#### Our RAG Application\n",
        "\n",
        "Let's spin up the graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "VnGthXpzzo-R"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import START, StateGraph\n",
        "from typing_extensions import TypedDict\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "class State(TypedDict):\n",
        "  question: str\n",
        "  context: list[Document]\n",
        "  response: str\n",
        "\n",
        "def retrieve(state: State) -> State:\n",
        "  retrieved_docs = qdrant_retriever.invoke(state[\"question\"])\n",
        "  return {\"context\" : retrieved_docs}\n",
        "\n",
        "def generate(state: State) -> State:\n",
        "  generator_chain = chat_prompt | openai_chat_model | StrOutputParser()\n",
        "  response = generator_chain.invoke({\"query\" : state[\"question\"], \"context\" : state[\"context\"]})\n",
        "  return {\"response\" : response}\n",
        "\n",
        "graph_builder = StateGraph(State)\n",
        "graph_builder = graph_builder.add_sequence([retrieve, generate])\n",
        "graph_builder.add_edge(START, \"retrieve\")\n",
        "rag_graph = graph_builder.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0KAPrtFMtRd"
      },
      "source": [
        "Let's get a visual understanding of our chain!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8ocIXNGMsue",
        "outputId": "eb34fc03-7052-4825-82e1-85489ac84e78"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<langgraph.graph.state.CompiledStateGraph object at 0x13363bfb0>"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rag_graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0C5CFRHOxtB"
      },
      "source": [
        "Let's test our chain out!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "JSDyVefDaue4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The provided context does not mention a specific date or details regarding when the Philippines gained independence. It discusses various historical influences and movements during the Spanish colonial period and the changes that occurred in governance without providing concrete information about the independence of the Philippines.'"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response = rag_graph.invoke({\"question\" : \"When did the Philippines gain independence?\"})\n",
        "response[\"response\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"background-color: #204B8E; color: white; padding: 10px; border-radius: 5px;\">\n",
        "\n",
        "### 🎯 Breakout Room - Group Discussion: \n",
        "\n",
        "Why did the model answer the question even when its not related to writings of Dr. Jose Rizal?\n",
        "\n",
        "How can you improve the prompt to respond only within the context?  \n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"background-color: #204B8E; color: white; padding: 10px; border-radius: 5px;\">\n",
        "\n",
        "### Observations :\n",
        "\n",
        "* By removing the system prompt to respond I don't know to out of context questions, it will make an attempt to answer still. But when including it, it responds strictly with I don't know.\n",
        "</span>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJtSdDsXZXam"
      },
      "source": [
        "## Task 3: Setting Up LangSmith (Extra! Extra!)\n",
        "\n",
        "Now that we have a chain - we're ready to get started with LangSmith!\n",
        "\n",
        "Create a Langsmith account here(https://smith.langchain.com/) and Setup your API key.\n",
        "\n",
        "We're going to go ahead and use the following `env` variables to get our notebook set up to start reporting.\n",
        "\n",
        "If all you needed was simple monitoring - this is all you would need to do!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "iqPdBXSBD4a-"
      },
      "outputs": [],
      "source": [
        "from uuid import uuid4\n",
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "PROJECT_NAME = f\"PSI AI Eng - DAY_3 - {uuid4().hex[0:8]}\"\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = PROJECT_NAME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ms4msyKLaIr6"
      },
      "source": [
        "### LangSmith API\n",
        "\n",
        "In order to use LangSmith - you will need an API key. You can sign up for a free account on [LangSmith's homepage!](https://www.langchain.com/langsmith)\n",
        "\n",
        "Once you have created your account, Take the navigation option for `Settings` then `API Keys` to create an API key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVq1EYngEMhV",
        "outputId": "5560a787-8114-4c72-b95a-6a776043427a"
      },
      "outputs": [],
      "source": [
        "from langchain.callbacks import LangChainTracer\n",
        "\n",
        "os.environ[\"LANGSMITH_API_KEY\"] = getpass('Enter your LangSmith API key: ')\n",
        "\n",
        "tracer = LangChainTracer()  \n",
        "tracer.project_name = PROJECT_NAME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qy0MMBLacXv"
      },
      "source": [
        "Let's test our our first generation!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3eoqBtBQERXP",
        "outputId": "fc1ea857-6784-4e7e-9fd4-94fbdd9f693a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Response:\n",
            " Capitan Tiago, whose full name is Don Santiago de los Santos, is a character portrayed as low in stature, with a clear complexion, a corpulent figure, and a full face. He appears younger than his actual age, likely between thirty and thirty-five years old. His countenance typically has a sanctified look, with a round head of ebony-black hair that is long in front and short behind. He is known to have small eyes without a Chinese slant and a slender nose. Despite a habit of chewing tobacco and buyo, which affects his mouth's symmetry, he maintains white teeth.\n",
            "\n",
            "In terms of social standing, Capitan Tiago is considered one of the richest landlords in Binondo and is also a significant planter due to his estates in Pampanga. He is characterized as a hospitable man, known for hosting dinners that garner much attention and anticipation, showcasing his status within the community (as illustrated in the accounts of his dinner invitations and interactions with others) (pages 54, 87, 478).\n",
            "\n",
            "Tracing Project name: PSI AI Eng - DAY_3 - cff6c4c3\n"
          ]
        }
      ],
      "source": [
        "result = rag_graph.invoke(\n",
        "    {\"question\": \"Who is Capitan Tiago?\"},\n",
        "    config={\"tags\": [\"Demo Run\"], \"callbacks\": [tracer]}\n",
        ")\n",
        "\n",
        "print(\"\\nResponse:\\n\", result['response'])\n",
        "print(\"\\nTracing Project name:\", tracer.project_name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZxABFzPr2ny"
      },
      "source": [
        "## Task 4: Examining the Trace in LangSmith!\n",
        "\n",
        "Head on over to your LangSmith web UI to check out how the trace looks in LangSmith!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c52o58AfsLK6"
      },
      "source": [
        "<div style=\"background-color: #204B8E; color: white; padding: 10px; border-radius: 5px;\">\n",
        "\n",
        "#### 🏗️ Activity #1:\n",
        "\n",
        "Include a screenshot of your trace and explain what it means.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"background-color: #204B8E; color: white; padding: 10px; border-radius: 5px;\">\n",
        "\n",
        "![Day3 Trace](images/trace.png)\n",
        "\n",
        "### Observations :\n",
        "\n",
        "* In the trace, you can quickly see processing time between each step. The detailed view also shows all the prompts used, the tokens used, and i/o costs.\n",
        "</span>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
